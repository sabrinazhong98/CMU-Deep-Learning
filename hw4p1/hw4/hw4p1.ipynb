{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QRRMJzfqFPk",
        "outputId": "4d617cbb-e031-4dca-8743-4c4ec0ba9221"
      },
      "source": [
        "#connect to gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd gdrive/MyDrive\n",
        "#%cd ../..\n",
        "!pwd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive\n",
            "/content/gdrive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxiZ42B4SwQ-"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tests\n",
        "#from tests import test_prediction, test_generation"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GIzAnNOqUOo",
        "outputId": "9afc02e7-f5b3-4603-ad12-1fd70b0a9b6c"
      },
      "source": [
        "%cd hw4p1/dataset"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/hw4p1/dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13aDHuEGqeym",
        "outputId": "25cd9abd-2087-45a0-8902-1246f3dc6a93"
      },
      "source": [
        "\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "experiments\t     prediction.npz\t  vocab.csv\t  wiki.valid.npy\n",
            "generation.npy\t     prediction_test.npz  vocab.npy\n",
            "generation_test.npy  tests.py\t\t  wiki.train.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5znxQhLSwRC"
      },
      "source": [
        "# load all that we need\n",
        "\n",
        "dataset = np.load('wiki.train.npy', allow_pickle=True)\n",
        "devset = np.load('wiki.valid.npy', allow_pickle=True)\n",
        "fixtures_pred = np.load('prediction.npz')  # dev\n",
        "fixtures_gen = np.load('generation.npy')  # dev\n",
        "fixtures_pred_test = np.load('prediction_test.npz')  # test\n",
        "fixtures_gen_test = np.load('generation_test.npy')  # test\n",
        "vocab = np.load('vocab.npy')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrmp_O35mM00"
      },
      "source": [
        "# data loader\n",
        "\n",
        "class LanguageModelDataLoader(DataLoader):\n",
        "    \"\"\"\n",
        "        TODO: Define data loader logic here\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, batch_size, shuffle=True):\n",
        "        \n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        \n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.dataset)\n",
        "        # concatenate your articles and build into batches\n",
        "        concats = np.concatenate(self.dataset)\n",
        "\n",
        "        p = np.random.random_sample()\n",
        "            \n",
        "        if p < 0.95:\n",
        "            seqlens = int(np.random.normal(70,5))\n",
        "        else:\n",
        "            seqlens = int(np.random.normal(35,5))\n",
        " \n",
        "        \n",
        "        concat_len = (len(concats)// seqlens) * seqlens\n",
        "        \n",
        "        x = torch.from_numpy(concats[:concat_len])\n",
        "        y = torch.from_numpy(concats[1: concat_len+1])\n",
        "\n",
        "        x_set = []\n",
        "        y_set = []\n",
        "        \n",
        "        idx = 0\n",
        "        while idx < concat_len:\n",
        "          x_set.append(x[idx: idx+seqlens])\n",
        "          y_set.append(y[idx: idx+seqlens])\n",
        "          idx += seqlens\n",
        "\n",
        "       \n",
        "        num_batch =len(x_set)//self.batch_size\n",
        "   \n",
        "        x_set = torch.stack(x_set)[:num_batch * self.batch_size]\n",
        "        y_set = torch.stack(y_set)[:num_batch * self.batch_size]\n",
        "\n",
        "        x_set = x_set.view(num_batch,self.batch_size, -1).long()\n",
        "        y_set = y_set.view(num_batch,self.batch_size, -1).long()\n",
        "\n",
        "        for l in range(len(x_set)):\n",
        "          current_x = x_set[l]\n",
        "          current_y = y_set[l]          \n",
        "          yield current_x, current_y\n",
        "        \n",
        "      "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhxCoYdOum5F"
      },
      "source": [
        "class LockedDropout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x, dropout=0.5):\n",
        "        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - dropout)\n",
        "        mask = Variable(m, requires_grad=False) / (1 - dropout)\n",
        "        mask = mask.expand_as(x)\n",
        "        return mask * x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nDTwWrnnqFQ"
      },
      "source": [
        "def embedded_dropout(embed, words, dropout=0.1, scale=None):\n",
        "  if dropout:\n",
        "    mask = embed.weight.data.new().resize_((embed.weight.size(0), 1)).bernoulli_(1 - dropout).expand_as(embed.weight) / (1 - dropout)\n",
        "    masked_embed_weight = mask * embed.weight\n",
        "\n",
        "  X = torch.nn.functional.embedding(words, masked_embed_weight,\n",
        "    -1, embed.max_norm, embed.norm_type,\n",
        "    embed.scale_grad_by_freq, embed.sparse\n",
        "  )\n",
        "  return X"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt-7YsTYSwRI"
      },
      "source": [
        "\n",
        "# model\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "        TODO: Define your model here\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, hidden = 1150, embed = 400):#embed = hidden\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed)\n",
        "        self.dropout = LockedDropout()\n",
        "        \n",
        "        #self.lstm = nn.LSTM(input_size = embed, hidden_size = hidden, \n",
        "         #                   num_layers=3, batch_first=True, dropout = 0.6)\n",
        "        \n",
        "        self.lstm1 = nn.LSTM(input_size = embed, hidden_size = hidden, \n",
        "                            num_layers=1, batch_first=True)\n",
        "                     \n",
        "        self.lstm2 = nn.LSTM(input_size = embed, hidden_size = hidden, \n",
        "                            num_layers=1, batch_first=True)\n",
        "        \n",
        "        self.lstm3 = nn.LSTM(input_size = embed, hidden_size = hidden, \n",
        "                            num_layers=1, batch_first=True)\n",
        "    \n",
        "        self.linear = nn.Linear(hidden, hidden*2)\n",
        "        self.linear2 = nn.Linear(hidden*2, hidden)\n",
        "        self.output = nn.Linear(hidden, vocab_size)\n",
        "        \n",
        "        self.hidsize = hidden\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        #weight tying\n",
        "        self.output.weight = self.embedding.weight\n",
        "   \n",
        "\n",
        "    def forward(self, x, h0 = None):\n",
        "        \n",
        "        # Feel free to add extra arguments to forward (like an argument to pass in the hiddens)\n",
        "        if  'array' in str(type(x)):\n",
        "          x = torch.tensor(x)\n",
        "  \n",
        "        x = x.to(device)\n",
        "\n",
        "        emb = embedded_dropout(self.embedding,x, dropout = 0.1)\n",
        "        raw_output = []\n",
        "        x = self.dropout(emb, 0.3)\n",
        "\n",
        "        #x, h0 = self.lstm(x, h0)       \n",
        "        hnew = []  \n",
        "        outputs = []\n",
        "        x,h0 = self.lstm1(x,h0)\n",
        "        x = self.dropout(x,0.4)\n",
        "        x,h0 = self.lstm2(x,h0)\n",
        "        x = self.dropout(x, 0.5)\n",
        "        x, h0 = self.lstm3(x,h0)\n",
        "\n",
        "\n",
        "        output = self.output(self.dropout(x))\n",
        "        \n",
        "        #print(output.shape)\n",
        "\n",
        "        return output.permute(0,2,1), raw_output\n",
        "        \n",
        "\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIvZOIfjSwRK"
      },
      "source": [
        "# model trainer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class LanguageModelTrainer:\n",
        "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
        "        \"\"\"\n",
        "            Use this class to train your model\n",
        "        \"\"\"\n",
        "        # feel free to add any other parameters here\n",
        "        self.model = model\n",
        "        self.loader = loader\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.predictions = []\n",
        "        self.predictions_test = []\n",
        "        self.generated_logits = []\n",
        "        self.generated = []\n",
        "        self.generated_logits_test = []\n",
        "        self.generated_test = []\n",
        "        self.epochs = 0\n",
        "        self.max_epochs = max_epochs\n",
        "        self.run_id = run_id\n",
        "        \n",
        "        # TODO: Define your optimizer and criterion here\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(),lr = 0.004,\n",
        "                                         weight_decay = 1.2e-6)\n",
        "       # self.scheduler =torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min', 0.5,1)\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=1, gamma=0.8)\n",
        "\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train() # set to training mode\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
        "           \n",
        "            epoch_loss += self.train_batch(inputs, targets)\n",
        "            \n",
        "\n",
        "        epoch_loss = epoch_loss / (batch_num + 1)\n",
        "        self.epochs += 1\n",
        "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, epoch_loss))\n",
        "        self.train_losses.append(epoch_loss)\n",
        "\n",
        "    def train_batch(self, inputs, targets):\n",
        "        \"\"\" \n",
        "            TODO: Define code for training a single batch of inputs\n",
        "\n",
        "        \n",
        "        \"\"\"\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        \n",
        "        targets = targets.to(device)\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        outputs, raw_output = self.model(inputs)\n",
        "        #print(outputs.shape, targets.shape)\n",
        "\n",
        "\n",
        "        loss = self.criterion(outputs, targets) #+ sum(0.6 * (raw[1:] - raw[:-1]).pow(2).mean() for raw in raw_output[-1:])\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "    \n",
        "        return loss\n",
        "\n",
        "    \n",
        "    def test(self):\n",
        "        # don't change these\n",
        "\n",
        "        self.model.eval() # set to eval mode\n",
        "        predictions = TestLanguageModel.prediction(fixtures_pred['inp'], self.model) # get predictions\n",
        "        self.predictions.append(predictions)\n",
        "        generated_logits = TestLanguageModel.generation(fixtures_gen, 10, self.model) # generated predictions for 10 words\n",
        "        generated_logits_test = TestLanguageModel.generation(fixtures_gen_test, 10, self.model)\n",
        "        nll = test_prediction(predictions, fixtures_pred['out'])  #changed\n",
        "        generated = test_generation(fixtures_gen, generated_logits, vocab) #changed\n",
        "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab) #changed\n",
        "        self.val_losses.append(nll)\n",
        "        \n",
        "        self.generated.append(generated)\n",
        "        self.generated_test.append(generated_test)\n",
        "        self.generated_logits.append(generated_logits)\n",
        "        self.generated_logits_test.append(generated_logits_test)\n",
        "        \n",
        "        # generate predictions for test data\n",
        "        predictions_test = TestLanguageModel.prediction(fixtures_pred_test['inp'], self.model) # get predictions\n",
        "        self.predictions_test.append(predictions_test)\n",
        "        self.scheduler.step()\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "          print(param_group['lr'])\n",
        "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, nll))\n",
        "        return nll\n",
        "\n",
        "    def save(self):\n",
        "        # don't change these\n",
        "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
        "        torch.save({'state_dict': self.model.state_dict()},\n",
        "            model_path)\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated_test[-1])\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPI7_kZRSwRN"
      },
      "source": [
        "class TestLanguageModel:\n",
        "    def prediction(inp, model):\n",
        "        \"\"\"\n",
        "            TODO: write prediction code here\n",
        "            \n",
        "            :param inp:\n",
        "            :return: a np.ndarray of logits\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            output,_ = model(inp)\n",
        "            output = output.permute(0,2,1)\n",
        "            \n",
        "        return output[:,-1].cpu().detach().numpy()\n",
        "\n",
        "        \n",
        "    def generation(inp, forward, model):\n",
        "        \"\"\"\n",
        "            TODO: write generation code here\n",
        "\n",
        "            Generate a sequence of words given a starting sequence.\n",
        "            :param inp: Initial sequence of words (batch size, length)\n",
        "            :param forward: number of additional words to generate\n",
        "            :return: generated words (batch size, forward)\n",
        "            \n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            gen = []\n",
        "            inp = torch.tensor(inp).to(device)\n",
        "            output,_ = model(inp)\n",
        "            output = output.permute(0,2,1)\n",
        "            \n",
        "\n",
        "            for f in range(forward):  \n",
        "              if f == 0:\n",
        "                w = torch.argmax(output, dim = 2)\n",
        "                w = w[:,len(w[0])-1].unsqueeze(1)\n",
        "                gen.append(w)\n",
        "              else:\n",
        "                w,_ = model(w)\n",
        "                w = w.permute(0,2,1)    \n",
        "                w = torch.argmax(w, dim = 2)\n",
        "                w = w[:,len(w[0])-1].unsqueeze(1)\n",
        "                gen.append(w)\n",
        "                \n",
        "            generated = torch.cat(gen, dim=1).cpu().detach().numpy()  \n",
        "        return  generated\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            gen = []\n",
        "            inp = torch.tensor(inp).to(device)\n",
        "            output,_ = model(inp)\n",
        "            output = output.permute(0,2,1)\n",
        "\n",
        "          \n",
        "            o = output[:,-1]\n",
        "            \n",
        "            w = torch.argmax(o, dim = 1).unsqueeze(1)\n",
        "            word_cat = torch.cat((inp, w), dim = 1)\n",
        "            gen.append(w)\n",
        "  \n",
        "            for f in range(forward -1):\n",
        "              out,_ = model(word_cat)\n",
        "              out = out.permute(0,2,1)\n",
        "              o = out[:,-1]\n",
        "              w = torch.argmax(o, dim = 1).unsqueeze(1)\n",
        "              word_cat = torch.cat((word_cat, w), dim = 1)\n",
        "             \n",
        "        return  word_cat[:,-forward:].cpu().numpy()"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiUrjbEjSwRQ"
      },
      "source": [
        "\n",
        "# TODO: define other hyperparameters here\n",
        "\n",
        "NUM_EPOCHS = 15\n",
        "BATCH_SIZE = 64\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HCVG5YISwRW",
        "scrolled": true,
        "outputId": "cae3d824-c7aa-4f23-bbff-ddc9c6fb118b"
      },
      "source": [
        "run_id = str(int(time.time()))\n",
        "if not os.path.exists('./experiments'):\n",
        "    os.mkdir('./experiments')\n",
        "os.mkdir('./experiments/%s' % run_id)\n",
        "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving models, predictions, and generated words to ./experiments/1620597432\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeYfzJtK19Gw"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def log_softmax(x, axis):\n",
        "    ret = x - np.max(x, axis=axis, keepdims=True)\n",
        "    lsm = np.log(np.sum(np.exp(ret), axis=axis, keepdims=True))\n",
        "    return ret - lsm\n",
        "\n",
        "\n",
        "def array_to_str(arr, vocab):\n",
        "    return \" \".join(vocab[a] for a in arr)\n",
        "\n",
        "\n",
        "def test_prediction(out, targ):\n",
        "    out = log_softmax(out, 1)\n",
        "    nlls = out[np.arange(out.shape[0]), targ]\n",
        "    nll = -np.mean(nlls)\n",
        "    return nll\n",
        "\n",
        "def test_generation(inp, pred, vocab):\n",
        "    outputs = u\"\"\n",
        "    for i in range(inp.shape[0]):\n",
        "        w1 = array_to_str(inp[i], vocab)\n",
        "        w2 = array_to_str(pred[i], vocab)\n",
        "        outputs += u\"Input | Output #{}: {} | {}\\n\".format(i, w1, w2)\n",
        "    return outputs"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbHH6zXTSwRa"
      },
      "source": [
        "\n",
        "model = LanguageModel(len(vocab), hidden =400, embed = 400).to(device)\n",
        "loader = LanguageModelDataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "trainer = LanguageModelTrainer(model=model, loader=loader, max_epochs=NUM_EPOCHS, run_id=run_id)\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D8wTJkBSwRc",
        "scrolled": true,
        "outputId": "d192bb43-9d24-41eb-b5fb-637df91f38f0"
      },
      "source": [
        "best_nll = 5\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    trainer.train()\n",
        "    nll = trainer.test()\n",
        "    #print(nll)\n",
        "  \n",
        "    if nll < best_nll:\n",
        "        best_nll = nll\n",
        "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
        "        trainer.save()\n",
        "    "
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [2/15]   Loss: 6.9594\n",
            "0.0032\n",
            "[VAL]  Epoch [2/15]   Loss: 5.7462\n",
            "[TRAIN]  Epoch [3/15]   Loss: 6.2466\n",
            "0.00256\n",
            "[VAL]  Epoch [3/15]   Loss: 5.4103\n",
            "[TRAIN]  Epoch [4/15]   Loss: 5.9893\n",
            "0.0020480000000000003\n",
            "[VAL]  Epoch [4/15]   Loss: 5.1841\n",
            "[TRAIN]  Epoch [5/15]   Loss: 5.8187\n",
            "0.0016384000000000004\n",
            "[VAL]  Epoch [5/15]   Loss: 5.0520\n",
            "[TRAIN]  Epoch [6/15]   Loss: 5.6902\n",
            "0.0013107200000000005\n",
            "[VAL]  Epoch [6/15]   Loss: 5.2533\n",
            "[TRAIN]  Epoch [7/15]   Loss: 5.5954\n",
            "0.0010485760000000005\n",
            "[VAL]  Epoch [7/15]   Loss: 4.9968\n",
            "Saving model, predictions and generated output for epoch 5 with NLL: 4.996788\n",
            "[TRAIN]  Epoch [8/15]   Loss: 5.4409\n",
            "0.0008388608000000005\n",
            "[VAL]  Epoch [8/15]   Loss: 4.9674\n",
            "Saving model, predictions and generated output for epoch 6 with NLL: 4.9673824\n",
            "[TRAIN]  Epoch [9/15]   Loss: 5.3780\n",
            "0.0006710886400000004\n",
            "[VAL]  Epoch [9/15]   Loss: 4.8460\n",
            "Saving model, predictions and generated output for epoch 7 with NLL: 4.846002\n",
            "[TRAIN]  Epoch [10/15]   Loss: 5.3147\n",
            "0.0005368709120000003\n",
            "[VAL]  Epoch [10/15]   Loss: 4.8365\n",
            "Saving model, predictions and generated output for epoch 8 with NLL: 4.836479\n",
            "[TRAIN]  Epoch [11/15]   Loss: 5.2752\n",
            "0.0004294967296000003\n",
            "[VAL]  Epoch [11/15]   Loss: 4.9984\n",
            "[TRAIN]  Epoch [12/15]   Loss: 5.2384\n",
            "0.00034359738368000027\n",
            "[VAL]  Epoch [12/15]   Loss: 4.8063\n",
            "Saving model, predictions and generated output for epoch 10 with NLL: 4.806265\n",
            "[TRAIN]  Epoch [13/15]   Loss: 5.2027\n",
            "0.00027487790694400024\n",
            "[VAL]  Epoch [13/15]   Loss: 4.8641\n",
            "[TRAIN]  Epoch [14/15]   Loss: 5.1856\n",
            "0.0002199023255552002\n",
            "[VAL]  Epoch [14/15]   Loss: 4.9630\n",
            "[TRAIN]  Epoch [15/15]   Loss: 5.1691\n",
            "0.00017592186044416018\n",
            "[VAL]  Epoch [15/15]   Loss: 4.7715\n",
            "Saving model, predictions and generated output for epoch 13 with NLL: 4.771451\n",
            "[TRAIN]  Epoch [16/15]   Loss: 5.1483\n",
            "0.00014073748835532815\n",
            "[VAL]  Epoch [16/15]   Loss: 4.6818\n",
            "Saving model, predictions and generated output for epoch 14 with NLL: 4.681836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "z2FmDqBCSwRf",
        "outputId": "e2d04bc0-d74d-46b9-b22d-439d2cf254d6"
      },
      "source": [
        "\n",
        "# Don't change these\n",
        "# plot training curves\n",
        "plt.figure()\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('NLL')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipdbmqaGSwRh",
        "outputId": "35085d11-0f7e-4668-9c26-0e1347ebb51a"
      },
      "source": [
        "print (trainer.generated[-1]) "
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input | Output #0: while the group was en route , but only three were ultimately able to attack . None of them were | not to . The two @-@ month line of .\n",
            "Input | Output #1: <unk> , where he remained on loan until 30 June 2010 . <eol> = = = Return to Manchester United | = = = <eol> On July May , the first\n",
            "Input | Output #2: 25 April 2013 , denoting shipments of 500 @,@ 000 copies . <eol> The song became One Direction 's fourth | @-@ selling single , \" The Year \" , was\n",
            "Input | Output #3: , and Bruce R. ) one daughter ( Wendy J. <unk> ) and two grandchildren , died in <unk> , | <unk> , <unk> , <unk> , <unk> ) , <unk>\n",
            "Input | Output #4: Warrior were examples of this type . Because their armor was so heavy , they could only carry a single | @-@ day @-@ field <unk> , and a few days\n",
            "Input | Output #5: the embassy at 1 : 49 and landed on Guam at 2 : 23 ; twenty minutes later , Ambassador | was also reported to have been used for the game\n",
            "Input | Output #6: <unk> , $ 96 million USD ) . Damage was heaviest in South Korea , notably where it moved ashore | . The <unk> , the <unk> , , and <unk>\n",
            "Input | Output #7: The <unk> were condemned as <unk> by <unk> , who saw the riots as hampering attempts to resolve the situation | . <eol> = = = = Kingdom = = =\n",
            "Input | Output #8: by a decision made by the War Office in mid @-@ 1941 , as it was considering the equipment to | be used as a <unk> . <eol> = = =\n",
            "Input | Output #9: Division crossed the <unk> at a number of places and climbed the hills quietly toward the 9th Infantry river line | . <eol> = = = = Hurricane = = =\n",
            "Input | Output #10: = <eol> = = = French VIII . Corps ( Corps <unk> ) = = = <eol> On 6 November | , the <unk> <unk> was the first son of in\n",
            "Input | Output #11: of the World from 9th Avenue \" . This is regarded as his most famous work . It is considered | a new <unk> of the <unk> , <unk> , <unk>\n",
            "Input | Output #12: — <unk> @-@ 10 , <unk> @-@ 12 , <unk> @-@ 16 , <unk> @-@ 17 — were all converted | to the <unk> . <eol> = = = <unk> =\n",
            "Input | Output #13: And now he has . \" <eol> = = Family = = <eol> <unk> lived 37 of his years in | the series , but the first time was the first\n",
            "Input | Output #14: Hell to which he has been condemned for <unk> . Eliot , in a letter to John <unk> dated 27 | May , and the <unk> of the <unk> , which\n",
            "Input | Output #15: Luoyang area , fulfilling his duties in domestic affairs . <eol> In the autumn of <unk> , he met Li | 's death in the , and the United States ,\n",
            "Input | Output #16: Power said they enjoyed Block Ball and its number of stages , but wondered how its eight <unk> of memory | was more . The film 's music was , and\n",
            "Input | Output #17: by Lloyd F. Lonergan . The cameraman was Jacques <unk> . <eol> = = Release and reception = = <eol> | <unk> ( <unk> ) , <unk> , <unk> , <unk>\n",
            "Input | Output #18: alone , the Austrians lost more than half their reserve artillery park , 6 @,@ 000 ( out of 8 | @,@ 000 @,@ 000 long tons ) . The .\n",
            "Input | Output #19: while attacking a ship at <unk> in the Dutch East Indies ; the loss was compounded by the fact that | the first three @-@ year @-@ old was were <unk>\n",
            "Input | Output #20: first raised in 2007 by the member of parliament ( MP ) for <unk> . The gangsters may have run | in the <unk> , which is the only of of\n",
            "Input | Output #21: Species are also non @-@ spiny <unk> and includes both large trees with stout stems up to 30 metres ( | 3 ft ) , and are the most common species\n",
            "Input | Output #22: \" : specific design issues with the building 's energy efficiency included the fact that the largest room in the | area . . The <unk> of the <unk> , which\n",
            "Input | Output #23: were reported to support over 300 @,@ 000 households in the Brazilian state of <unk> in 2005 , and in | May 2011 , the was also the first to be\n",
            "Input | Output #24: port . <unk> in Vietnam also warned for the potential of heavy rainfall due to the dissipating Tropical Depression <unk> | , the German and Ottoman force was completed in January\n",
            "Input | Output #25: T @-@ numbers in their tropical cyclone products . The following example is from discussion number 3 of Tropical Depression | At the <unk> , the second highway was to be\n",
            "Input | Output #26: South Australia hosted the three @-@ game semi @-@ final series against the New South Wales <unk> . Both teams | were also called the the most popular and most of\n",
            "Input | Output #27: Perth from contention and secured the last finals spot for the <unk> . <eol> = = = Statistical leaders = | = = <eol> The film was the first to be\n",
            "Input | Output #28: deemed it an \" amazing pop song \" , lauding the group 's falsetto and its \" head @-@ <unk> | \" . <eol> The film 's original <unk> , <unk>\n",
            "Input | Output #29: , but began patrolling the English Channel after <unk> @-@ 6 pioneered a route past British <unk> nets and mines | . The <unk> , the <unk> , , and the\n",
            "Input | Output #30: production executives to let him direct . He had already discussed the film with <unk> and Cohen , and felt | that was the <unk> in of the series , and\n",
            "Input | Output #31: and Nick <unk> at Studio <unk> in Los Angeles , California , and was released on August 1 , 2006 | . <eol> = = = = = <unk> = =\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2y9CNVls8HN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}