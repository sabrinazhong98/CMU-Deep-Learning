{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QRRMJzfqFPk",
        "outputId": "4d617cbb-e031-4dca-8743-4c4ec0ba9221"
      },
      "source": [
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd gdrive/MyDrive\n",
        "!pwd\n",
        "\"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive\n",
            "/content/gdrive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxiZ42B4SwQ-"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tests\n",
        "#from tests import test_prediction, test_generation"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GIzAnNOqUOo",
        "outputId": "9afc02e7-f5b3-4603-ad12-1fd70b0a9b6c"
      },
      "source": [
        "#%cd hw4p1/dataset"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/hw4p1/dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5znxQhLSwRC"
      },
      "source": [
        "# load all that we need\n",
        "\n",
        "dataset = np.load('wiki.train.npy', allow_pickle=True)\n",
        "devset = np.load('wiki.valid.npy', allow_pickle=True)\n",
        "fixtures_pred = np.load('prediction.npz')  # dev\n",
        "fixtures_gen = np.load('generation.npy')  # dev\n",
        "fixtures_pred_test = np.load('prediction_test.npz')  # test\n",
        "fixtures_gen_test = np.load('generation_test.npy')  # test\n",
        "vocab = np.load('vocab.npy')"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrmp_O35mM00"
      },
      "source": [
        "# data loader\n",
        "\n",
        "class LanguageModelDataLoader(DataLoader):\n",
        "    \"\"\"\n",
        "        TODO: Define data loader logic here\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, batch_size, shuffle=True):\n",
        "        \n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        \n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.dataset)\n",
        "        # concatenate your articles and build into batches\n",
        "        concats = np.concatenate(self.dataset)\n",
        "\n",
        "        p = np.random.random_sample()\n",
        "            \n",
        "        if p < 0.95:\n",
        "            seqlens = int(np.random.normal(70,5))\n",
        "        else:\n",
        "            seqlens = int(np.random.normal(35,5))\n",
        " \n",
        "        \n",
        "        concat_len = (len(concats)// seqlens) * seqlens\n",
        "        \n",
        "        x = torch.from_numpy(concats[:concat_len])\n",
        "        y = torch.from_numpy(concats[1: concat_len+1])\n",
        "\n",
        "        x_set = []\n",
        "        y_set = []\n",
        "        \n",
        "        idx = 0\n",
        "        while idx < concat_len:\n",
        "          x_set.append(x[idx: idx+seqlens])\n",
        "          y_set.append(y[idx: idx+seqlens])\n",
        "          idx += seqlens\n",
        "\n",
        "       \n",
        "        num_batch =len(x_set)//self.batch_size\n",
        "   \n",
        "        x_set = torch.stack(x_set)[:num_batch * self.batch_size]\n",
        "        y_set = torch.stack(y_set)[:num_batch * self.batch_size]\n",
        "\n",
        "        x_set = x_set.view(num_batch,self.batch_size, -1).long()\n",
        "        y_set = y_set.view(num_batch,self.batch_size, -1).long()\n",
        "\n",
        "        for l in range(len(x_set)):\n",
        "          current_x = x_set[l]\n",
        "          current_y = y_set[l]          \n",
        "          yield current_x, current_y\n",
        "        \n",
        "      "
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhxCoYdOum5F"
      },
      "source": [
        "class LockedDropout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x, dropout=0.5):\n",
        "        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - dropout)\n",
        "        mask = Variable(m, requires_grad=False) / (1 - dropout)\n",
        "        mask = mask.expand_as(x)\n",
        "        return mask * x"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt-7YsTYSwRI"
      },
      "source": [
        "class LanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "        TODO: Define your model here\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, hidden = 1150, embed = 400):#embed = hidden\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed)\n",
        "        self.dropout = LockedDropout()\n",
        "        \n",
        "        self.lstm1 = nn.LSTM(input_size = embed, hidden_size = hidden, \n",
        "                            num_layers=1, batch_first=True)\n",
        "                     \n",
        "        self.lstm2 = nn.LSTM(input_size = embed, hidden_size = hidden, \n",
        "                            num_layers=1, batch_first=True)\n",
        "        \n",
        "        self.lstm3 = nn.LSTM(input_size = embed, hidden_size = hidden, \n",
        "                            num_layers=1, batch_first=True)\n",
        "    \n",
        "        self.output = nn.Linear(hidden, vocab_size)\n",
        "        self.hidsize = hidden\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        #weight tying\n",
        "        self.output.weight = self.embedding.weight\n",
        "   \n",
        "\n",
        "    def forward(self, x, h0 = None):\n",
        "        \n",
        "        # Feel free to add extra arguments to forward (like an argument to pass in the hiddens)\n",
        "        if  'array' in str(type(x)):\n",
        "          x = torch.tensor(x)\n",
        "  \n",
        "        x = x.to(device)\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x, 0.3)\n",
        "        raw_output = []\n",
        "\n",
        "        hnew = []  \n",
        "        outputs = []\n",
        "        x,h0 = self.lstm1(x,h0)\n",
        "        x = self.dropout(x,0.4)\n",
        "        x,h0 = self.lstm2(x,h0)\n",
        "        x = self.dropout(x, 0.4)\n",
        "        x, h0 = self.lstm3(x,h0)\n",
        "      \n",
        "        output = self.output(self.dropout(x))\n",
        "        \n",
        "        #print(output.shape)\n",
        "\n",
        "        return output.permute(0,2,1), raw_output\n",
        "        \n",
        "\n"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIvZOIfjSwRK"
      },
      "source": [
        "# model trainer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class LanguageModelTrainer:\n",
        "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
        "        \"\"\"\n",
        "            Use this class to train your model\n",
        "        \"\"\"\n",
        "        # feel free to add any other parameters here\n",
        "        self.model = model\n",
        "        self.loader = loader\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.predictions = []\n",
        "        self.predictions_test = []\n",
        "        self.generated_logits = []\n",
        "        self.generated = []\n",
        "        self.generated_logits_test = []\n",
        "        self.generated_test = []\n",
        "        self.epochs = 0\n",
        "        self.max_epochs = max_epochs\n",
        "        self.run_id = run_id\n",
        "        \n",
        "        # TODO: Define your optimizer and criterion here\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(),lr = 0.003,\n",
        "                                         weight_decay = 1.2e-6)\n",
        "        self.scheduler =torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min', 0.5,1)\n",
        "\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train() # set to training mode\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
        "           \n",
        "            epoch_loss += self.train_batch(inputs, targets)\n",
        "            \n",
        "\n",
        "        epoch_loss = epoch_loss / (batch_num + 1)\n",
        "        self.epochs += 1\n",
        "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, epoch_loss))\n",
        "        self.train_losses.append(epoch_loss)\n",
        "\n",
        "    def train_batch(self, inputs, targets):\n",
        "        \"\"\" \n",
        "            TODO: Define code for training a single batch of inputs\n",
        "\n",
        "        \n",
        "        \"\"\"\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        \n",
        "        targets = targets.to(device)\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        outputs, raw_output = self.model(inputs)\n",
        "\n",
        "        loss = self.criterion(outputs, targets) \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "    \n",
        "        return loss\n",
        "\n",
        "    \n",
        "    def test(self):\n",
        "        # don't change these\n",
        "\n",
        "        self.model.eval() # set to eval mode\n",
        "        predictions = TestLanguageModel.prediction(fixtures_pred['inp'], self.model) # get predictions\n",
        "        self.predictions.append(predictions)\n",
        "        generated_logits = TestLanguageModel.generation(fixtures_gen, 10, self.model) # generated predictions for 10 words\n",
        "        generated_logits_test = TestLanguageModel.generation(fixtures_gen_test, 10, self.model)\n",
        "        nll = test_prediction(predictions, fixtures_pred['out'])  #changed\n",
        "        generated = test_generation(fixtures_gen, generated_logits, vocab) #changed\n",
        "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab) #changed\n",
        "        self.val_losses.append(nll)\n",
        "        \n",
        "        self.generated.append(generated)\n",
        "        self.generated_test.append(generated_test)\n",
        "        self.generated_logits.append(generated_logits)\n",
        "        self.generated_logits_test.append(generated_logits_test)\n",
        "        \n",
        "        # generate predictions for test data\n",
        "        predictions_test = TestLanguageModel.prediction(fixtures_pred_test['inp'], self.model) # get predictions\n",
        "        self.predictions_test.append(predictions_test)\n",
        "        self.scheduler.step(nll)\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "          print(param_group['lr'])\n",
        "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, nll))\n",
        "        return nll\n",
        "\n",
        "    def save(self):\n",
        "        # don't change these\n",
        "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
        "        torch.save({'state_dict': self.model.state_dict()},\n",
        "            model_path)\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated_test[-1])\n"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPI7_kZRSwRN"
      },
      "source": [
        "class TestLanguageModel:\n",
        "    def prediction(inp, model):\n",
        "        \"\"\"\n",
        "            TODO: write prediction code here\n",
        "            \n",
        "            :param inp:\n",
        "            :return: a np.ndarray of logits\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            output,_ = model(inp)\n",
        "            output = output.permute(0,2,1)\n",
        "            \n",
        "        return output[:,-1].cpu().detach().numpy()\n",
        "\n",
        "        \n",
        "    def generation(inp, forward, model):\n",
        "        \"\"\"\n",
        "            TODO: write generation code here\n",
        "\n",
        "            Generate a sequence of words given a starting sequence.\n",
        "            :param inp: Initial sequence of words (batch size, length)\n",
        "            :param forward: number of additional words to generate\n",
        "            :return: generated words (batch size, forward)\n",
        "        \n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            gen = []\n",
        "            inp = torch.tensor(inp).to(device)\n",
        "            output,_ = model(inp)\n",
        "            output = output.permute(0,2,1)\n",
        "\n",
        "          \n",
        "            o = output[:,-1]\n",
        "            \n",
        "            w = torch.argmax(o, dim = 1).unsqueeze(1)\n",
        "            word_cat = torch.cat((inp, w), dim = 1)\n",
        "            gen.append(w)\n",
        "  \n",
        "            for f in range(forward -1):\n",
        "              out,_ = model(word_cat)\n",
        "              out = out.permute(0,2,1)\n",
        "              o = out[:,-1]\n",
        "              w = torch.argmax(o, dim = 1).unsqueeze(1)\n",
        "              word_cat = torch.cat((word_cat, w), dim = 1)\n",
        "             \n",
        "        return  word_cat[:,-forward:].cpu().numpy()"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiUrjbEjSwRQ"
      },
      "source": [
        "\n",
        "# TODO: define other hyperparameters here\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 128\n"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HCVG5YISwRW",
        "scrolled": true,
        "outputId": "48d52c5e-c36d-4570-83d0-380a435c9068"
      },
      "source": [
        "run_id = str(int(time.time()))\n",
        "if not os.path.exists('./experiments'):\n",
        "    os.mkdir('./experiments')\n",
        "os.mkdir('./experiments/%s' % run_id)\n",
        "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving models, predictions, and generated words to ./experiments/1620608345\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeYfzJtK19Gw"
      },
      "source": [
        "import numpy as np\n",
        "\"\"\"this is from the test.py\n",
        "def log_softmax(x, axis):\n",
        "    ret = x - np.max(x, axis=axis, keepdims=True)\n",
        "    lsm = np.log(np.sum(np.exp(ret), axis=axis, keepdims=True))\n",
        "    return ret - lsm\n",
        "\n",
        "\n",
        "def array_to_str(arr, vocab):\n",
        "    return \" \".join(vocab[a] for a in arr)\n",
        "\n",
        "\n",
        "def test_prediction(out, targ):\n",
        "    out = log_softmax(out, 1)\n",
        "    nlls = out[np.arange(out.shape[0]), targ]\n",
        "    nll = -np.mean(nlls)\n",
        "    return nll\n",
        "\n",
        "def test_generation(inp, pred, vocab):\n",
        "    outputs = u\"\"\n",
        "    for i in range(inp.shape[0]):\n",
        "        w1 = array_to_str(inp[i], vocab)\n",
        "        w2 = array_to_str(pred[i], vocab)\n",
        "        outputs += u\"Input | Output #{}: {} | {}\\n\".format(i, w1, w2)\n",
        "    return outputs\n",
        "\"\"\" "
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbHH6zXTSwRa"
      },
      "source": [
        "\n",
        "model = LanguageModel(len(vocab), hidden =400, embed = 400).to(device)\n",
        "loader = LanguageModelDataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "trainer = LanguageModelTrainer(model=model, loader=loader, max_epochs=NUM_EPOCHS, run_id=run_id)\n"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7D8wTJkBSwRc",
        "scrolled": true,
        "outputId": "d5ded1ff-965f-48b9-8078-89482541088d"
      },
      "source": [
        "best_nll = 4.5\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    trainer.train()\n",
        "    nll = trainer.test()\n",
        "    #print(nll)\n",
        "  \n",
        "    if nll < best_nll:\n",
        "        best_nll = nll\n",
        "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
        "        trainer.save()\n",
        "    "
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [17/15]   Loss: 4.8968\n",
            "0.003\n",
            "[VAL]  Epoch [17/15]   Loss: 4.5460\n",
            "[TRAIN]  Epoch [18/15]   Loss: 4.8659\n",
            "0.003\n",
            "[VAL]  Epoch [18/15]   Loss: 4.5731\n",
            "[TRAIN]  Epoch [19/15]   Loss: 4.8205\n",
            "0.0015\n",
            "[VAL]  Epoch [19/15]   Loss: 4.6618\n",
            "[TRAIN]  Epoch [20/15]   Loss: 4.7124\n",
            "0.0015\n",
            "[VAL]  Epoch [20/15]   Loss: 4.4907\n",
            "Saving model, predictions and generated output for epoch 3 with NLL: 4.490713\n",
            "[TRAIN]  Epoch [21/15]   Loss: 4.6622\n",
            "0.0015\n",
            "[VAL]  Epoch [21/15]   Loss: 4.5088\n",
            "[TRAIN]  Epoch [22/15]   Loss: 4.6388\n",
            "0.00075\n",
            "[VAL]  Epoch [22/15]   Loss: 4.5728\n",
            "[TRAIN]  Epoch [23/15]   Loss: 4.5698\n",
            "0.00075\n",
            "[VAL]  Epoch [23/15]   Loss: 4.5685\n",
            "[TRAIN]  Epoch [24/15]   Loss: 4.5503\n",
            "0.000375\n",
            "[VAL]  Epoch [24/15]   Loss: 4.5970\n",
            "[TRAIN]  Epoch [25/15]   Loss: 4.5112\n",
            "0.000375\n",
            "[VAL]  Epoch [25/15]   Loss: 4.6429\n",
            "[TRAIN]  Epoch [26/15]   Loss: 4.4998\n",
            "0.0001875\n",
            "[VAL]  Epoch [26/15]   Loss: 4.6050\n",
            "[TRAIN]  Epoch [27/15]   Loss: 4.4799\n",
            "0.0001875\n",
            "[VAL]  Epoch [27/15]   Loss: 4.6474\n",
            "[TRAIN]  Epoch [28/15]   Loss: 4.4737\n",
            "9.375e-05\n",
            "[VAL]  Epoch [28/15]   Loss: 4.5664\n",
            "[TRAIN]  Epoch [29/15]   Loss: 4.4629\n",
            "9.375e-05\n",
            "[VAL]  Epoch [29/15]   Loss: 4.6339\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-151-f9ca86ac1ead>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbest_nll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mnll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#print(nll)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-144-c1bd7ed2fbac>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-144-c1bd7ed2fbac>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#+ sum(0.6 * (raw[1:] - raw[:-1]).pow(2).mean() for raw in raw_output[-1:])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "z2FmDqBCSwRf",
        "outputId": "1a51d5e8-2b7b-4e1c-9adc-b1373270b6fa"
      },
      "source": [
        "\n",
        "# Don't change these\n",
        "# plot training curves\n",
        "plt.figure()\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('NLL')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipdbmqaGSwRh",
        "outputId": "35085d11-0f7e-4668-9c26-0e1347ebb51a"
      },
      "source": [
        "print (trainer.generated[-1]) "
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input | Output #0: while the group was en route , but only three were ultimately able to attack . None of them were | not to . The two @-@ month line of .\n",
            "Input | Output #1: <unk> , where he remained on loan until 30 June 2010 . <eol> = = = Return to Manchester United | = = = <eol> On July May , the first\n",
            "Input | Output #2: 25 April 2013 , denoting shipments of 500 @,@ 000 copies . <eol> The song became One Direction 's fourth | @-@ selling single , \" The Year \" , was\n",
            "Input | Output #3: , and Bruce R. ) one daughter ( Wendy J. <unk> ) and two grandchildren , died in <unk> , | <unk> , <unk> , <unk> , <unk> ) , <unk>\n",
            "Input | Output #4: Warrior were examples of this type . Because their armor was so heavy , they could only carry a single | @-@ day @-@ field <unk> , and a few days\n",
            "Input | Output #5: the embassy at 1 : 49 and landed on Guam at 2 : 23 ; twenty minutes later , Ambassador | was also reported to have been used for the game\n",
            "Input | Output #6: <unk> , $ 96 million USD ) . Damage was heaviest in South Korea , notably where it moved ashore | . The <unk> , the <unk> , , and <unk>\n",
            "Input | Output #7: The <unk> were condemned as <unk> by <unk> , who saw the riots as hampering attempts to resolve the situation | . <eol> = = = = Kingdom = = =\n",
            "Input | Output #8: by a decision made by the War Office in mid @-@ 1941 , as it was considering the equipment to | be used as a <unk> . <eol> = = =\n",
            "Input | Output #9: Division crossed the <unk> at a number of places and climbed the hills quietly toward the 9th Infantry river line | . <eol> = = = = Hurricane = = =\n",
            "Input | Output #10: = <eol> = = = French VIII . Corps ( Corps <unk> ) = = = <eol> On 6 November | , the <unk> <unk> was the first son of in\n",
            "Input | Output #11: of the World from 9th Avenue \" . This is regarded as his most famous work . It is considered | a new <unk> of the <unk> , <unk> , <unk>\n",
            "Input | Output #12:  <unk> @-@ 10 , <unk> @-@ 12 , <unk> @-@ 16 , <unk> @-@ 17  were all converted | to the <unk> . <eol> = = = <unk> =\n",
            "Input | Output #13: And now he has . \" <eol> = = Family = = <eol> <unk> lived 37 of his years in | the series , but the first time was the first\n",
            "Input | Output #14: Hell to which he has been condemned for <unk> . Eliot , in a letter to John <unk> dated 27 | May , and the <unk> of the <unk> , which\n",
            "Input | Output #15: Luoyang area , fulfilling his duties in domestic affairs . <eol> In the autumn of <unk> , he met Li | 's death in the , and the United States ,\n",
            "Input | Output #16: Power said they enjoyed Block Ball and its number of stages , but wondered how its eight <unk> of memory | was more . The film 's music was , and\n",
            "Input | Output #17: by Lloyd F. Lonergan . The cameraman was Jacques <unk> . <eol> = = Release and reception = = <eol> | <unk> ( <unk> ) , <unk> , <unk> , <unk>\n",
            "Input | Output #18: alone , the Austrians lost more than half their reserve artillery park , 6 @,@ 000 ( out of 8 | @,@ 000 @,@ 000 long tons ) . The .\n",
            "Input | Output #19: while attacking a ship at <unk> in the Dutch East Indies ; the loss was compounded by the fact that | the first three @-@ year @-@ old was were <unk>\n",
            "Input | Output #20: first raised in 2007 by the member of parliament ( MP ) for <unk> . The gangsters may have run | in the <unk> , which is the only of of\n",
            "Input | Output #21: Species are also non @-@ spiny <unk> and includes both large trees with stout stems up to 30 metres ( | 3 ft ) , and are the most common species\n",
            "Input | Output #22: \" : specific design issues with the building 's energy efficiency included the fact that the largest room in the | area . . The <unk> of the <unk> , which\n",
            "Input | Output #23: were reported to support over 300 @,@ 000 households in the Brazilian state of <unk> in 2005 , and in | May 2011 , the was also the first to be\n",
            "Input | Output #24: port . <unk> in Vietnam also warned for the potential of heavy rainfall due to the dissipating Tropical Depression <unk> | , the German and Ottoman force was completed in January\n",
            "Input | Output #25: T @-@ numbers in their tropical cyclone products . The following example is from discussion number 3 of Tropical Depression | At the <unk> , the second highway was to be\n",
            "Input | Output #26: South Australia hosted the three @-@ game semi @-@ final series against the New South Wales <unk> . Both teams | were also called the the most popular and most of\n",
            "Input | Output #27: Perth from contention and secured the last finals spot for the <unk> . <eol> = = = Statistical leaders = | = = <eol> The film was the first to be\n",
            "Input | Output #28: deemed it an \" amazing pop song \" , lauding the group 's falsetto and its \" head @-@ <unk> | \" . <eol> The film 's original <unk> , <unk>\n",
            "Input | Output #29: , but began patrolling the English Channel after <unk> @-@ 6 pioneered a route past British <unk> nets and mines | . The <unk> , the <unk> , , and the\n",
            "Input | Output #30: production executives to let him direct . He had already discussed the film with <unk> and Cohen , and felt | that was the <unk> in of the series , and\n",
            "Input | Output #31: and Nick <unk> at Studio <unk> in Los Angeles , California , and was released on August 1 , 2006 | . <eol> = = = = = <unk> = =\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}