{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of hw2p2_final_redo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9KahJv0LQuq"
      },
      "source": [
        "# setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tkz3zfv7NxDM"
      },
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "!kaggle competitions download -c 11785-spring2021-hw2p2s1-face-classification\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1Kvat37Lgxa"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "import PIL\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from torchvision.transforms import Compose, RandomCrop, ToTensor, ToPILImage, CenterCrop, Resize\n",
        "import matplotlib.pyplot as plt\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo9ZYUmsMJ6Y"
      },
      "source": [
        "\n",
        "normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "transforms = torchvision.transforms.Compose([\n",
        "\n",
        "    torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    #torchvision.transforms.RandomRotation(20, resample=PIL.Image.BILINEAR),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "train_dataset = torchvision.datasets.ImageFolder(root='train_data', \n",
        "                                                 transform = transforms)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, \n",
        "                                               shuffle=True, num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HNcCWBh6dGD"
      },
      "source": [
        "transforms = torchvision.transforms.Compose([\n",
        "                                             torchvision.transforms.ToTensor(),\n",
        "                                             normalize\n",
        "])\n",
        "\n",
        "val_dataset = torchvision.datasets.ImageFolder(root='val_data', \n",
        "                                                 transform= transforms)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=128, \n",
        "                                               shuffle=False, num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2l6kG_OLdqn"
      },
      "source": [
        "# training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTNn7HF_Nqep"
      },
      "source": [
        "\n",
        "class SELayer(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bli2LiupLmGE"
      },
      "source": [
        "class SimpleResidualBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels ,stride=1, downsample = False):\n",
        "        super(SimpleResidualBlock, self).__init__()\n",
        "        self.downsample = downsample\n",
        "        self.expansion = 1\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels,kernel_size = 1,stride = 1, padding = 0, bias = False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(out_channels,out_channels,kernel_size = 3,stride = stride, padding = 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.se = SELayer(out_channels, out_channels)\n",
        "        \n",
        "        self.shortcut = nn.Sequential()\n",
        "        if downsample == False:\n",
        "            self.shortcut = nn.Identity()\n",
        "        else:\n",
        "     \n",
        "            self.shortcut = nn.Conv2d(in_channels,out_channels, kernel_size=1, stride=stride, bias = False)\n",
        "            self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "       # self.relu = nn.ReLU(inplace = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "      \n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.se(out)\n",
        "\n",
        "        if self.downsample == True:\n",
        "          shortcut = self.bn3(self.shortcut(x))\n",
        "        else:\n",
        "          shortcut = self.shortcut(x)\n",
        "       \n",
        "        out = self.relu(out + shortcut)\n",
        "        #out = out + shortcut\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTQwJ7_HEho5"
      },
      "source": [
        "class Bottleneck(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels ,stride=1, downsample = False):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.downsample = downsample\n",
        "        self.expansion = 4\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels,kernel_size = 1,stride = 1, padding = 0, bias = False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(out_channels,out_channels,kernel_size = 3,stride = stride, padding = 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels*self.expansion, kernel_size = 1, stride = 1, bias = False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
        "        \n",
        "        self.shortcut = nn.Sequential()\n",
        "        if downsample == False:\n",
        "            self.shortcut = nn.Identity()\n",
        "        else:\n",
        "  \n",
        "            self.shortcut = nn.Conv2d(out_channels,out_channels*self.expansion, kernel_size=1, stride=stride, bias = False)\n",
        "            self.bn4 = nn.BatchNorm2d(out_channels*self.expansion)\n",
        "       # self.relu = nn.ReLU(inplace = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "      \n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample == True:\n",
        "          shortcut = self.bn4(self.shortcut(x))\n",
        "        else:\n",
        "          shortcut = self.shortcut(x)\n",
        "       \n",
        "        out = self.relu(out + shortcut)\n",
        "        #out = out + shortcut\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzNbQBoILnae"
      },
      "source": [
        "\n",
        "class ResNet(torch.nn.Module):\n",
        "  def __init__(self, block, layers,num_classes, feat_dim):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.in_channels = 32\n",
        "    self.conv1 = nn.Conv2d(3,32, kernel_size = 3, stride = 1, padding = 1, bias = False )\n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "    \n",
        "    self.relu = nn.ReLU()\n",
        "    #self.maxpool = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 1)\n",
        "\n",
        " \n",
        "    #for resnet layers\n",
        "    self.layer1 = self.sublayer(block, layers[0], channel_out = 32, stride = 1)\n",
        "    self.layer2 = self.sublayer(block, layers[1], channel_out = 48, stride = 1)\n",
        "    self.layer3 = self.sublayer(block, layers[2], channel_out = 96, stride = 1)\n",
        "    self.layer4 = self.sublayer(block, layers[3], channel_out = 192, stride = 2) \n",
        "    self.bn2 = nn.BatchNorm2d(192*block(2000,4000).expansion)\n",
        "   \n",
        "    self.avgpool =  nn.AdaptiveAvgPool2d((1, 1))\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.dropout = nn.Dropout(p= 0.4)\n",
        "    self.linear = nn.Linear(self.in_channels, feat_dim)\n",
        "    \n",
        "    self.linear_output = nn.Linear(192*block(2000, 4000).expansion, num_classes)\n",
        "    \n",
        "    \n",
        "  \n",
        "  def sublayer(self, block , num_block, channel_out, stride ):\n",
        "   \n",
        "    expansion = block(64,64,1).expansion\n",
        "    layers = []\n",
        "    for s in range(num_block):\n",
        "      if s == 0:\n",
        "        if expansion*channel_out != self.in_channels: # test if the in channel and out channel are the same\n",
        "          layers.append(block(self.in_channels, channel_out, downsample = True))\n",
        "        else:\n",
        "          layers.append(block(channel_out, channel_out))\n",
        "        self.in_channels = channel_out\n",
        "      else:\n",
        "        layers.append(block(self.in_channels*expansion, channel_out, stride, downsample = True))\n",
        "    self.in_channels = channel_out * expansion\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x, return_embedding = False):\n",
        "\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    #x = self.maxpool(x)\n",
        "\n",
        "    x = self.layer1(x)  \n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.layer4(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.avgpool(x)\n",
        "   \n",
        "    x = self.flatten(x)\n",
        "    x = self.dropout(x)\n",
        "    \n",
        "    embedding_out = self.relu(self.linear(x))\n",
        "\n",
        "    output = self.linear_output(x)\n",
        "    if return_embedding:\n",
        "      return embedding_out, output\n",
        "    else:\n",
        "      return output\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ccs0vUSLqS1"
      },
      "source": [
        "\n",
        "\n",
        "def ResNet34(num_classes, feat_dim):\n",
        "  return ResNet(SimpleResidualBlock, [3,4,6,3],  num_classes, feat_dim)\n",
        "\n",
        "def ResNet50(num_classes, feat_dim):\n",
        "  return ResNet(Bottleneck, [3,4,6,3],  num_classes, feat_dim)\n",
        "\n",
        "def ResNet50(num_classes, feat_dim):\n",
        "  return ResNet(Bottleneck, [3,8,36,3],  num_classes, feat_dim)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vs82rDjmxsvU"
      },
      "source": [
        "def training(epoch, network):\n",
        "  cropsize_list = [24, 36, 48, 54, 64]\n",
        "  standard = 0.3\n",
        "  \n",
        "\n",
        "  for epoch in range(numEpochs):\n",
        "      \n",
        "      network.train()\n",
        "      avg_loss = 0.0\n",
        "      num_correct = 0\n",
        "\n",
        "      for batch_num, (x, y) in enumerate(train_dataloader):\n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          x, y = x.to(device), y.to(device)\n",
        "\n",
        "          #crop the image\n",
        "          cropsize = random.choice(cropsize_list) #randomly center crop\n",
        "          center_crop = torchvision.transforms.CenterCrop(cropsize)\n",
        "          crop_image = center_crop(x)\n",
        "          \n",
        "          #put images to network\n",
        "          outputs = network(crop_image)\n",
        "\n",
        "          loss = criterion(outputs, y.long())\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          \n",
        "          avg_loss += loss.item()\n",
        "          if batch_num % 99 == 1:\n",
        "              print('loss', avg_loss/100)\n",
        "              avg_loss = 0.0\n",
        "       \n",
        "          #print(torch.argmax(outputs, axis = 1)[0],y[0])\n",
        "          num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "          del x\n",
        "          del y\n",
        "          del crop_image\n",
        "          del outputs\n",
        "          torch.cuda.empty_cache()\n",
        "      \n",
        "      \n",
        "      print('Epoch: {}, Training Accuracy: {:.2f}'.format(epoch, num_correct / len(train_dataset)))\n",
        "      print('train loss',avg_loss)\n",
        "      \n",
        "      # Validate\n",
        "      avg_loss = 0\n",
        "      num_correct = 0\n",
        "      network.eval()\n",
        "      with torch.no_grad():\n",
        "        cropsize_list = [24, 36, 48, 54, 64]\n",
        "\n",
        "        for batch_num, (x, y) in enumerate(val_dataloader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            decision = []\n",
        "\n",
        "            for cropsize in cropsize_list:\n",
        "              center_crop = torchvision.transforms.CenterCrop(cropsize)\n",
        "              crop_image = center_crop(x)\n",
        "              outputs = network(crop_image)\n",
        "              decision.append(outputs)\n",
        "              del outputs\n",
        "            \n",
        "            decision = sum(decision) / len(cropsize_list)\n",
        "            num_correct += (torch.argmax(decision, axis=1) == y).sum().item()\n",
        "            del x\n",
        "            del y\n",
        "      print('val loss', avg_loss)\n",
        "      print('Epoch: {}, Validation Accuracy: {:.2f}'.format(epoch, num_correct / len(val_dataset)))\n",
        "      acc = num_correct / len(val_dataset)\n",
        "      scheduler.step(acc)\n",
        "      if acc >= standard:\n",
        "        torch.save(network,'resnet50_last_1')\n",
        "        print('saving...', acc)\n",
        "        standard = acc \n",
        "\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73mKjFEd_StJ"
      },
      "source": [
        "# tuning hyperparameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Rj4nWZc1oIP"
      },
      "source": [
        "## resnet 34 best\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCHAMffHNOdX"
      },
      "source": [
        "# yields 0.82 of accuracy\n",
        "# set up model\n",
        "num_classes = len(train_dataset.classes)\n",
        "feat_dim = 1200\n",
        "\n",
        "#model\n",
        "network_34 = ResNet34(num_classes, feat_dim=1200)\n",
        "network_34 = network_34.to(device)\n",
        "\n",
        "#hyperparameter\n",
        "numEpochs = 25\n",
        "in_features = 3 #RGB channels\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(network_34.parameters(), lr=0.01, weight_decay=5e-5, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "training(numEpochs, network_34)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19SomIB7lFOx"
      },
      "source": [
        "## resnet 50 best"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4gZgzqpK4oX"
      },
      "source": [
        "# yields 0.83 of accuracy\n",
        "# set up model\n",
        "num_classes = len(train_dataset.classes)\n",
        "feat_dim = 1200\n",
        "\n",
        "#model\n",
        "network_50 = ResNet50(num_classes, feat_dim=1200)\n",
        "network_50 = network_50.to(device)\n",
        "\n",
        "#hyperparameter\n",
        "numEpochs = 25\n",
        "in_features = 3 #RGB channels\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(network_50.parameters(), lr=0.01, weight_decay=5e-5, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "training(numEpochs, network_50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIAYy6sKlVcf"
      },
      "source": [
        "## resnet 152 best"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8nqqyjVM_3r"
      },
      "source": [
        "# set up model\n",
        "num_classes = len(train_dataset.classes)\n",
        "feat_dim = 1200\n",
        "\n",
        "#model\n",
        "network_152 = ResNet152(num_classes, feat_dim=1200)\n",
        "network_152 = network_152.to(device)\n",
        "\n",
        "#hyperparameter\n",
        "numEpochs = 25\n",
        "in_features = 3 #RGB channels\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(network_152.parameters(), lr=0.01, weight_decay=5e-5, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "training(numEpochs, network_152)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUZhfraHuNv6"
      },
      "source": [
        "## majority vote"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nk1P0VHbuQkG",
        "outputId": "b97c8206-2962-4b3f-a0b7-1989138b18fd"
      },
      "source": [
        "# Validate\n",
        "avg_loss = 0\n",
        "num_correct = 0\n",
        "network_34.eval()\n",
        "network_50.eval()\n",
        "network_152.eval()\n",
        "with torch.no_grad():\n",
        "  cropsize_list = [24, 36, 48, 54, 64]\n",
        "\n",
        "  for batch_num, (x, y) in enumerate(val_dataloader):\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      decision_34 = []\n",
        "      decision_50 = []\n",
        "      decision_152 = []\n",
        "\n",
        "      for cropsize in cropsize_list:\n",
        "        center_crop = torchvision.transforms.CenterCrop(cropsize)\n",
        "        crop_image = center_crop(x)\n",
        "        outputs_34 = network_34(crop_image)\n",
        "        outputs_50 = network_50(crop_image)\n",
        "        outputs_152 = network_152(crop_image)\n",
        "        decision_34.append(outputs_34)\n",
        "        decision_50.append(outputs_50)\n",
        "        decision_152.append(outputs_152)\n",
        "\n",
        "        del outputs_34\n",
        "        del outputs_50\n",
        "        del outputs_152\n",
        "      \n",
        "      decision_34 = sum(decision_34) / len(cropsize_list)\n",
        "      class_34 = torch.argmax(decision_34, axis = 1)\n",
        "      decision_50 = sum(decision_50) / len(cropsize_list)\n",
        "      class_50 = torch.argmax(decision_50, axis = 1)\n",
        "      decision_152 = sum(decision_152) / len(cropsize_list)\n",
        "      class_152 = torch.argmax(decision_152, axis = 1)\n",
        "\n",
        "      #find majority vote\n",
        "      final_class = []\n",
        "      for i in range(len(class_34)):\n",
        "        all_results = [int(class_34[i]),int(class_50[i]),int(class_152[i])]\n",
        "        dict1 = dict(Counter(all_results))\n",
        "        sorted_keys = sorted(dict1, key=dict1.get, reverse=True)\n",
        "        final_class.append(sorted_keys[0])\n",
        "        \n",
        "\n",
        "      final_class = torch.tensor(final_class)\n",
        "      final_class = final_class.to(device)\n",
        "      num_correct += (final_class == y).sum().item()\n",
        "\n",
        "      del x\n",
        "      del y\n",
        "      del decision_34\n",
        "      del decision_50\n",
        "      del decision_152\n",
        "\n",
        "print('val loss', avg_loss)\n",
        "print('Epoch: {}, Validation Accuracy: {:.2f}'.format(0, num_correct / len(val_dataset)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "val loss 0\n",
            "Epoch: 0, Validation Accuracy: 0.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_LuPRMyFelo"
      },
      "source": [
        "# testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbkvUZ1yFhPc"
      },
      "source": [
        "#read the test image\n",
        "\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from PIL import Image\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, file_list):\n",
        "        self.file_list = file_list\n",
        "        self.imgfiles = [f for f in listdir(self.file_list) if isfile(join(self.file_list, f))]\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgfiles)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        fname = self.file_list + '/'+ self.imgfiles[index]\n",
        "        \n",
        "        img = Image.open(fname)\n",
        "        img = torchvision.transforms.ToTensor()(img)\n",
        "        img = normalize(img)\n",
        "      \n",
        "\n",
        "        return img,self.imgfiles[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLJqtU36GeT_"
      },
      "source": [
        "testset = ImageDataset('test_data')\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=64, \n",
        "                                               shuffle=False, num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljKe0IDU1bht"
      },
      "source": [
        "# Validate\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "avg_loss = 0\n",
        "num_correct = 0\n",
        "network_34.eval()\n",
        "network_50.eval()\n",
        "network_152.eval()\n",
        "preds_submit = []\n",
        "names = []\n",
        "with torch.no_grad():\n",
        "  cropsize_list = [24, 36, 48, 54, 64]\n",
        "\n",
        "  for batch_num, (x, name) in enumerate(test_loader):\n",
        "      x = x.to(device)\n",
        "      decision_34 = []\n",
        "      decision_50 = []\n",
        "      decision_152 = []\n",
        "      names.append(name)\n",
        "\n",
        "      for cropsize in cropsize_list:\n",
        "        center_crop = torchvision.transforms.CenterCrop(cropsize)\n",
        "        crop_image = center_crop(x)\n",
        "        outputs_34 = network_34(crop_image)\n",
        "        outputs_50 = network_50(crop_image)\n",
        "        outputs_152 = network_152(crop_image)\n",
        "        decision_34.append(outputs_34)\n",
        "        decision_50.append(outputs_50)\n",
        "        decision_152.append(outputs_152)\n",
        "\n",
        "        del outputs_34\n",
        "        del outputs_50\n",
        "        del outputs_152\n",
        "      \n",
        "      decision_34 = sum(decision_34) / len(cropsize_list)\n",
        "      class_34 = torch.argmax(decision_34, axis = 1)\n",
        "      decision_50 = sum(decision_50) / len(cropsize_list)\n",
        "      class_50 = torch.argmax(decision_50, axis = 1)\n",
        "      decision_152 = sum(decision_152) / len(cropsize_list)\n",
        "      class_152 = torch.argmax(decision_152, axis = 1)\n",
        "\n",
        "      #find majority vote\n",
        "      final_class = []\n",
        "      for i in range(len(class_34)):\n",
        "        all_results = [int(class_34[i]),int(class_50[i]),int(class_152[i])]\n",
        "        dict1 = dict(Counter(all_results))\n",
        "        sorted_keys = sorted(dict1, key=dict1.get, reverse=True)\n",
        "\n",
        "        \n",
        "        final_class.append(sorted_keys[0])\n",
        "      preds_submit.append(final_class)\n",
        "\n",
        "      del x\n",
        "      del name\n",
        "      del decision_34\n",
        "      del decision_50\n",
        "      del decision_152\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCeDP_5fJ3EC"
      },
      "source": [
        "network.eval()\n",
        "preds_total = []\n",
        "names = []\n",
        "with torch.no_grad():\n",
        "  for X_test, name in test_loader:\n",
        "   # print(name)\n",
        "    names.append(name)\n",
        "    X_test = X_test.to(device='cuda')\n",
        "    y_hat = network(X_test)\n",
        "    \n",
        "    y_hat = y_hat.cpu()\n",
        "    y_hat = y_hat.detach().numpy()\n",
        "    \n",
        "    preds = np.argmax(y_hat, axis = 1)\n",
        "   \n",
        "    preds = preds.reshape(-1,1)\n",
        "    preds_total.append(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYEUUIMCNbC8"
      },
      "source": [
        "dictclass = train_dataset.class_to_idx\n",
        "dictclass.keys()\n",
        "dictclass.values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgY2812K2q-H"
      },
      "source": [
        "finalnames = []\n",
        "for k in names:\n",
        "  #print(k)\n",
        "\n",
        "  for n in range(0,len(k)):\n",
        "    #print(k[n])\n",
        "\n",
        "    finalnames.append(k[n])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9UDamSWLYDX",
        "outputId": "3dd01b99-7788-468c-acf9-3b99793b2f53"
      },
      "source": [
        "\n",
        "preds_final = []\n",
        "for i in preds_submit:\n",
        "  for t in i:\n",
        "    \n",
        "    add = [key for key, val in dictclass.items() if val == t]\n",
        "    #print(add[0], t)\n",
        "    preds_final.append(int(add[0]))\n",
        "len(preds_final)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo6gOqp5LglC"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({'id':finalnames, \"label\":preds_final})\n",
        "df.to_csv(r\"submission_redo.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DjfPyHKCWKV"
      },
      "source": [
        "del train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cN84_hGcbCE"
      },
      "source": [
        "# task2 pre-processing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld1FSsoDU0Xc"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkc-kdQgYnfw"
      },
      "source": [
        "from PIL import Image\n",
        "import PIL.ImageOps  \n",
        "import pandas as pd\n",
        "dir = 'verification_pairs_val.txt'\n",
        "compare = pd.read_csv(dir, header = None, delimiter = \"\\t\")\n",
        "text = []\n",
        "for c in compare[0]:\n",
        "  i1, i2, l = c.split(\" \")\n",
        "  text.append([i1,i2,l])\n",
        "\n",
        "  \n",
        "class TrainDataset(Dataset):\n",
        "    \n",
        "    def __init__(self,imageFolderDataset,text, test = False):\n",
        "        \n",
        "        \n",
        "        self.imageFolderDataset = imageFolderDataset    \n",
        "        self.image = [[t[0],t[1]] for t in text]\n",
        "        self.label = [t[2] for t in text]\n",
        "        self.test = test\n",
        "        self.normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "\n",
        "        dirname0 =  text[index][0]\n",
        "        dirname1 =  text[index][1]\n",
        "        label = text[index][2]\n",
        "       \n",
        "        img0 = Image.open(dirname0)\n",
        "        img1 = Image.open(dirname1)\n",
        "\n",
        "        img0 = torchvision.transforms.ToTensor()(img0)\n",
        "        img1 = torchvision.transforms.ToTensor()(img1)\n",
        "\n",
        "        img0 = self.normalize(img0)\n",
        "        img1 = self.normalize(img1)\n",
        "\n",
        "    \n",
        "\n",
        "        if self.test == False:\n",
        "          l = torch.from_numpy(np.array(label, dtype = np.float32))\n",
        "          return img0, img1, l\n",
        "        else:\n",
        "          return img0, img1 ,0\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4npTWy9YqsD"
      },
      "source": [
        "\n",
        "\n",
        "train_dataset = TrainDataset(dir, text)\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                        shuffle=False,\n",
        "                        num_workers=0,\n",
        "                        batch_size=1 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgmEMcewEX6n"
      },
      "source": [
        "# task2 Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGP4rBBC9dC9"
      },
      "source": [
        "compute_sim = nn.CosineSimilarity(dim=0)\n",
        "#print(\"CS b/n two images of class 0: {:.4f}\".format(compute_sim(feats_0, feats_1)))\n",
        "def testVerify(model_34,model_50,model_152, train_dataloader):\n",
        "    similarity = np.array([])\n",
        "    true = np.array([])\n",
        "    cropsize_list = [24, 36, 48, 54, 64]\n",
        "    \n",
        "    model_34.eval()\n",
        "    model_50.eval()\n",
        "    model_152.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (img0, img1, targets) in enumerate(train_dataloader):\n",
        "            img0, img1, targets = img0.cuda(), img1.cuda(), targets.cuda()\n",
        "            \n",
        "\n",
        "            #decisions_0 = []\n",
        "            #decisions_1 = []\n",
        "            decisions_34_0 = []\n",
        "            decisions_50_0 = []\n",
        "            decisions_152_0 = []\n",
        "            decisions_34_1 = []\n",
        "            decisions_50_1 = []\n",
        "            decisions_152_1 = []\n",
        "\n",
        "            for cropsize in cropsize_list:\n",
        "              center_crop = torchvision.transforms.CenterCrop(cropsize)\n",
        "              crop_img0 = center_crop(img0)\n",
        "              crop_img1 = center_crop(img1)\n",
        "\n",
        "              # resnet34\n",
        "              feats_34_0 = model_34(crop_img0, return_embedding=False).squeeze(0)\n",
        "              feats_34_1 = model_34(crop_img1, return_embedding=False).squeeze(0)\n",
        "              decisions_34_0.append(feats_34_0)\n",
        "              decisions_34_1.append(feats_34_1)\n",
        "\n",
        "              # resnet50\n",
        "              feats_50_0 = model_50(crop_img0, return_embedding=False).squeeze(0)\n",
        "              feats_50_1 = model_50(crop_img1, return_embedding=False).squeeze(0)\n",
        "              decisions_50_0.append(feats_50_0)\n",
        "              decisions_50_1.append(feats_50_1)\n",
        "\n",
        "              # resnet152\n",
        "              feats_152_0 = model_152(crop_img0, return_embedding=False).squeeze(0)\n",
        "              feats_152_1 = model_152(crop_img1, return_embedding=False).squeeze(0)\n",
        "              decisions_152_0.append(feats_152_0)\n",
        "              decisions_152_1.append(feats_152_1)\n",
        "\n",
        "\n",
        "\n",
        "              del crop_img0\n",
        "              del crop_img1\n",
        "              del feats_34_0\n",
        "              del feats_34_1\n",
        "              del feats_50_0\n",
        "              del feats_50_1\n",
        "              del feats_152_0\n",
        "              del feats_152_1\n",
        "\n",
        "            decisions_34_0 = sum(decisions_34_0) / len(cropsize_list)\n",
        "            decisions_34_1 = sum(decisions_34_1) / len(cropsize_list)\n",
        "            decisions_50_0 = sum(decisions_50_0) / len(cropsize_list)\n",
        "            decisions_50_1 = sum(decisions_50_1) / len(cropsize_list)\n",
        "            decisions_152_0 = sum(decisions_152_0) / len(cropsize_list)\n",
        "            decisions_152_1 = sum(decisions_152_1) / len(cropsize_list)\n",
        "\n",
        "            decision_0 = sum([decisions_34_0,decisions_50_0,decisions_152_0])/3\n",
        "            decision_1 = sum([decisions_34_1,decisions_50_1,decisions_152_1])/3\n",
        "\n",
        "      \n",
        "            sim = compute_sim(decision_0,decision_1)\n",
        "                     \n",
        "            similarity = np.concatenate((similarity, sim.cpu().numpy().reshape(-1)))\n",
        "\n",
        "            true = np.concatenate((true, targets.cpu().numpy().reshape(-1)))\n",
        "            \n",
        "            del img0\n",
        "            del img1\n",
        "            del targets\n",
        "     \n",
        "    return similarity, true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yW7y2gZfgp5"
      },
      "source": [
        "compute_sim = nn.CosineSimilarity(dim=0)\n",
        "#print(\"CS b/n two images of class 0: {:.4f}\".format(compute_sim(feats_0, feats_1)))\n",
        "def testVerify(model, train_dataloader):\n",
        "    similarity = np.array([])\n",
        "    true = np.array([])\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (img0, img1, targets) in enumerate(train_dataloader):\n",
        "            img0, img1, targets = img0.cuda(), img1.cuda(), targets.cuda()\n",
        "          \n",
        "          \n",
        "            feats_0 = model(img0, return_embedding=False).squeeze(0)\n",
        "            feats_1 = model(img1, return_embedding=False).squeeze(0)\n",
        "      \n",
        "            sim = compute_sim(feats_0,feats_1)\n",
        "                     \n",
        "            similarity = np.concatenate((similarity, sim.cpu().numpy().reshape(-1)))\n",
        "\n",
        "            true = np.concatenate((true, targets.cpu().numpy().reshape(-1)))\n",
        "            \n",
        "            del img0\n",
        "            del img1\n",
        "            del targets\n",
        "    return similarity, true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnk4oVc8Kvoa"
      },
      "source": [
        "compute_sim = nn.CosineSimilarity(dim=0)\n",
        "#print(\"CS b/n two images of class 0: {:.4f}\".format(compute_sim(feats_0, feats_1)))\n",
        "def testVerify(model_50, train_dataloader):\n",
        "    similarity = np.array([])\n",
        "    true = np.array([])\n",
        "    cropsize_list = [24, 36, 48, 54, 64]\n",
        "\n",
        "    model_50.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (img0, img1, targets) in enumerate(train_dataloader):\n",
        "            img0, img1, targets = img0.cuda(), img1.cuda(), targets.cuda()\n",
        "       \n",
        "            decisions_50_0 = []\n",
        "            decisions_50_1 = []\n",
        "\n",
        "            for cropsize in cropsize_list:\n",
        "              center_crop = torchvision.transforms.CenterCrop(cropsize)\n",
        "              crop_img0 = center_crop(img0)\n",
        "              crop_img1 = center_crop(img1)\n",
        "\n",
        "              # resnet50\n",
        "              feats_50_0 = model_50(crop_img0, return_embedding=False).squeeze(0)\n",
        "              feats_50_1 = model_50(crop_img1, return_embedding=False).squeeze(0)\n",
        "              decisions_50_0.append(feats_50_0)\n",
        "              decisions_50_1.append(feats_50_1)\n",
        "\n",
        "              del crop_img0\n",
        "              del crop_img1\n",
        "              del feats_50_0\n",
        "              del feats_50_1\n",
        "\n",
        "            decisions_50_0 = sum(decisions_50_0) / len(cropsize_list)\n",
        "            decisions_50_1 = sum(decisions_50_1) / len(cropsize_list)\n",
        "      \n",
        "            sim = compute_sim(decisions_50_0,decisions_50_1)\n",
        "                     \n",
        "            similarity = np.concatenate((similarity, sim.cpu().numpy().reshape(-1)))\n",
        "\n",
        "            true = np.concatenate((true, targets.cpu().numpy().reshape(-1)))\n",
        "            \n",
        "            del img0\n",
        "            del img1\n",
        "            del targets\n",
        "     \n",
        "    return similarity, true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOg6uz9YNpkm"
      },
      "source": [
        "#%cd .. \n",
        "#%cd 11785-spring2021-hw2p2s2-face-verification\n",
        "similarity, true = testVerify(network_50, train_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qiz6wsC0cuNd",
        "outputId": "3db13725-1740-46fc-9ad8-23a86e0ed801"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "auc = roc_auc_score(true, similarity)\n",
        "print(auc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.931781866230351\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYL8P2haGIIJ"
      },
      "source": [
        "# task2 test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQP0i74yXdpb",
        "outputId": "25605993-1eb1-46b3-e290-c714feaf5917"
      },
      "source": [
        "%cd .. \n",
        "%cd 11785-spring2021-hw2p2s2-face-verification\n",
        "#network = torch.load('resnet_0.7x2')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/competitions\n",
            "/content/gdrive/My Drive/competitions/11785-spring2021-hw2p2s2-face-verification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMqeKlJMo1yA"
      },
      "source": [
        "dir = 'verification_pairs_test.txt'\n",
        "text = []\n",
        "compare = pd.read_csv(dir, header = None, delimiter = \"\\t\")\n",
        "for c in compare[0]:\n",
        "  i1, i2 = c.split(\" \")\n",
        "  text.append([i1,i2,0])\n",
        "\n",
        "test_dataset = TrainDataset(dir, text, test = True)\n",
        "test_dataloader = DataLoader(test_dataset,\n",
        "                        shuffle=False,\n",
        "                        num_workers=0,\n",
        "                        batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dndedocDs4lz"
      },
      "source": [
        "\n",
        "def testVerify_final(model, train_dataloader):\n",
        "    similarity = np.array([])\n",
        "    true = np.array([])\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (img0, img1, targets) in enumerate(train_dataloader):\n",
        "            img0, img1, targets = img0.cuda(), img1.cuda(), targets.cuda()\n",
        "            # find cos similarity between embeddings\n",
        "            #print(img0.shape)\n",
        "\n",
        "          \n",
        "            feats_0 = model(img0, return_embedding=True)[0].squeeze(0)\n",
        "            feats_1 = model(img1, return_embedding=True)[0].squeeze(0)\n",
        "      \n",
        "            sim = compute_sim(feats_0,feats_1)\n",
        "           # print('sim', sim)\n",
        "\n",
        "            similarity = np.concatenate((similarity, sim.cpu().numpy().reshape(-1)))\n",
        "    \n",
        "            \n",
        "            del img0\n",
        "            del img1\n",
        "            del targets\n",
        "    return similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1bpa4lHPsuq"
      },
      "source": [
        "compute_sim = nn.CosineSimilarity(dim=0)\n",
        "#print(\"CS b/n two images of class 0: {:.4f}\".format(compute_sim(feats_0, feats_1)))\n",
        "def testVerify_final(model_34,model_50,model_152, train_dataloader):\n",
        "    similarity = np.array([])\n",
        "    true = np.array([])\n",
        "    cropsize_list = [24, 36, 48, 54, 64]\n",
        "    \n",
        "    model_34.eval()\n",
        "    model_50.eval()\n",
        "    model_152.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (img0, img1, targets) in enumerate(train_dataloader):\n",
        "            print(batch_idx)\n",
        "            img0, img1, targets = img0.cuda(), img1.cuda(), targets.cuda()\n",
        "            \n",
        "            decisions_34_0 = []\n",
        "            decisions_50_0 = []\n",
        "            decisions_152_0 = []\n",
        "            decisions_34_1 = []\n",
        "            decisions_50_1 = []\n",
        "            decisions_152_1 = []\n",
        "\n",
        "            for cropsize in cropsize_list:\n",
        "              center_crop = torchvision.transforms.CenterCrop(cropsize)\n",
        "              crop_img0 = center_crop(img0)\n",
        "              crop_img1 = center_crop(img1)\n",
        "\n",
        "              # resnet34\n",
        "              feats_34_0 = model_34(crop_img0, return_embedding=False).squeeze(0)\n",
        "              feats_34_1 = model_34(crop_img1, return_embedding=False).squeeze(0)\n",
        "              decisions_34_0.append(feats_34_0)\n",
        "              decisions_34_1.append(feats_34_1)\n",
        "\n",
        "              # resnet50\n",
        "              feats_50_0 = model_50(crop_img0, return_embedding=False).squeeze(0)\n",
        "              feats_50_1 = model_50(crop_img1, return_embedding=False).squeeze(0)\n",
        "              decisions_50_0.append(feats_50_0)\n",
        "              decisions_50_1.append(feats_50_1)\n",
        "\n",
        "              # resnet152\n",
        "              feats_152_0 = model_152(crop_img0, return_embedding=False).squeeze(0)\n",
        "              feats_152_1 = model_152(crop_img1, return_embedding=False).squeeze(0)\n",
        "              decisions_152_0.append(feats_152_0)\n",
        "              decisions_152_1.append(feats_152_1)\n",
        "\n",
        "\n",
        "\n",
        "              del crop_img0\n",
        "              del crop_img1\n",
        "              del feats_34_0\n",
        "              del feats_34_1\n",
        "              del feats_50_0\n",
        "              del feats_50_1\n",
        "              del feats_152_0\n",
        "              del feats_152_1\n",
        "\n",
        "            decisions_34_0 = sum(decisions_34_0) / len(cropsize_list)\n",
        "            decisions_34_1 = sum(decisions_34_1) / len(cropsize_list)\n",
        "            decisions_50_0 = sum(decisions_50_0) / len(cropsize_list)\n",
        "            decisions_50_1 = sum(decisions_50_1) / len(cropsize_list)\n",
        "            decisions_152_0 = sum(decisions_152_0) / len(cropsize_list)\n",
        "            decisions_152_1 = sum(decisions_152_1) / len(cropsize_list)\n",
        "\n",
        "            decision_0 = sum([decisions_34_0,decisions_50_0,decisions_152_0])/3\n",
        "            decision_1 = sum([decisions_34_1,decisions_50_1,decisions_152_1])/3\n",
        "\n",
        "      \n",
        "            sim = compute_sim(decision_0,decision_1)\n",
        "                     \n",
        "            similarity = np.concatenate((similarity, sim.cpu().numpy().reshape(-1)))\n",
        "\n",
        "            \n",
        "            del img0\n",
        "            del img1\n",
        "            del targets\n",
        "     \n",
        "    return similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4XbcpLDNuDh"
      },
      "source": [
        "similarity = testVerify_final(network_34,network_50,network_152, test_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dcf6piQ4yidT"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({'Id':compare[0], \"Category\":similarity})\n",
        "df.to_csv(r\"submission4.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xUyiPrqy4Yh",
        "outputId": "7d2e51f6-7f5d-441f-c29a-bf75ee2744dc"
      },
      "source": [
        "!kaggle competitions submit -c hw2p2-summer2021 -f submission4.csv -m \"Message\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "100% 4.03M/4.03M [00:06<00:00, 653kB/s]\n",
            "Successfully submitted to HW2p2_summer2021"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_zAtBpA9G4s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}