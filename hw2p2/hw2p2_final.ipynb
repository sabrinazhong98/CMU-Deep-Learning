{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw2p2_final",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9KahJv0LQuq"
      },
      "source": [
        "# setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M47lTNQIK_r-",
        "outputId": "a9d29647-3f6c-4397-fe6e-3d3b3a9cf738"
      },
      "source": [
        "\n",
        "#!pip install -q --upgrade ipython\n",
        "#!pip install -q --upgrade ipykernel\n",
        "#connect to gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd gdrive/MyDrive\n",
        "#%cd ../..\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
           
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIgjYs5hdPK_",
        "outputId": "37b82cec-1c34-47a7-e092-e536a185663c"
      },
      "source": [
        "%cd competitions/11785-spring2021-hw2p2s1-face-classification"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/competitions/11785-spring2021-hw2p2s1-face-classification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2Qyuo6-Y2r-",
        "outputId": "f29ccaf1-39f4-4a3f-860e-c7e58ff7d19a"
      },
      "source": [
        "!pip install kaggle\n",
        "!mkdir .kaggle\n",
        "import json\n",
        "token = {\"username\":\"yuzhongfinally\",\"key\":\"da5ea7094c4adb8f29251a2e3b240da7\"}\n",
        "with open('/content/gdrive/MyDrive/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n",
        "!chmod 600 /content/gdrive/MyDrive/.kaggle/kaggle.json\n",
        "!cp /content/gdrive/MyDrive/.kaggle/kaggle.json /root/.kaggle/\n",
        "!kaggle config set -n path -v /content/gdrive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
          
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWzvKBs-Y_qa",
        "outputId": "7b87d9d2-543a-4812-c070-c093c3f409ef"
      },
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "!kaggle competitions download -c 11785-spring2021-hw2p2s1-face-classification\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kg0smAMzQ6sX",
        "outputId": "7f4e24ce-9ad0-4918-87c3-05d95c2e97d9"
      },
      "source": [
        "%cd competitions/11785-spring2021-hw2p2s1-face-classification\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnfm09zPLYiL",
        "outputId": "ba7ad99d-865c-439a-a4c5-c2d415bde04d"
      },
      "source": [
        "#%cd competitions\n",
        "!pwd\n",
        "!unzip 11785-spring2021-hw2p2s1-face-classification.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
      
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1Kvat37Lgxa"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PF62p0aXWh6",
        "outputId": "a2e3bdfd-477e-4689-a28f-eac094576ff4"
      },
      "source": [
        "%cd 11785-spring2021-hw2p2s1-face-classification\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '11785-spring2021-hw2p2s1-face-classification'\n",
            "/content/gdrive/MyDrive/competitions/11785-spring2021-hw2p2s1-face-classification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo9ZYUmsMJ6Y"
      },
      "source": [
        "import PIL\n",
        "transforms = torchvision.transforms.Compose([\n",
        "\n",
        "    torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    #torchvision.transforms.RandomRotation(20, resample=PIL.Image.BILINEAR),\n",
        "    torchvision.transforms.ToTensor()\n",
        "])\n",
        "train_dataset = torchvision.datasets.ImageFolder(root='train_data', \n",
        "                                                 transform = transforms)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, \n",
        "                                               shuffle=True, num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HNcCWBh6dGD"
      },
      "source": [
        "val_dataset = torchvision.datasets.ImageFolder(root='val_data', \n",
        "                                                 transform= torchvision.transforms.ToTensor())\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=128, \n",
        "                                               shuffle=False, num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2l6kG_OLdqn"
      },
      "source": [
        "# training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bli2LiupLmGE"
      },
      "source": [
        "class SimpleResidualBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels ,stride=1, downsample = False):\n",
        "        super(SimpleResidualBlock, self).__init__()\n",
        "        self.downsample = downsample\n",
        "        self.expansion = 1\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels,kernel_size = 1,stride = 1, padding = 0, bias = False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p= 0.1)\n",
        "        self.conv2 = nn.Conv2d(out_channels,out_channels,kernel_size = 3,stride = stride, padding = 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        self.shortcut = nn.Sequential()\n",
        "        if downsample == False:\n",
        "            self.shortcut = nn.Identity()\n",
        "        else:\n",
        "     \n",
        "            self.shortcut = nn.Conv2d(in_channels,out_channels, kernel_size=1, stride=stride, bias = False)\n",
        "            self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "       # self.relu = nn.ReLU(inplace = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "      \n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample == True:\n",
        "          shortcut = self.bn3(self.shortcut(x))\n",
        "        else:\n",
        "          shortcut = self.shortcut(x)\n",
        "        #print(shortcut.shape)\n",
        "       \n",
        "        out = self.relu(out + shortcut)\n",
        "        #out = out + shortcut\n",
        "        \n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzNbQBoILnae"
      },
      "source": [
        "\n",
        "class ResNet(torch.nn.Module):\n",
        "  def __init__(self, block, layers,num_classes, feat_dim):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.in_channels = 32\n",
        "    self.conv1 = nn.Conv2d(3,32, kernel_size = 3, stride = 1, padding = 1, bias = False )\n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "    \n",
        "    self.relu = nn.ReLU()\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 1)\n",
        "\n",
        " \n",
        "    #for resnet layers\n",
        "    self.layer1 = self.sublayer(block, layers[0], channel_out = 32, stride = 1)\n",
        "    self.layer2 = self.sublayer(block, layers[1], channel_out = 48, stride = 1)\n",
        "    self.layer3 = self.sublayer(block, layers[2], channel_out = 96, stride = 1)\n",
        "    self.layer4 = self.sublayer(block, layers[3], channel_out = 192, stride = 2) \n",
        "    self.bn2 = nn.BatchNorm2d(192*block(2000,4000).expansion)\n",
        "   \n",
        "    self.avgpool =  nn.AdaptiveAvgPool2d((1, 1))\n",
        "    self.flatten = nn.Flatten()\n",
        "    \n",
        "    self.linear = nn.Linear(self.in_channels, feat_dim)\n",
        "    \n",
        "    self.linear_output = nn.Linear(192*block(2000, 4000).expansion, num_classes)\n",
        "    \n",
        "    \n",
        "  \n",
        "  def sublayer(self, block , num_block, channel_out, stride ):\n",
        "   \n",
        "    expansion = block(64,64,1).expansion\n",
        "    layers = []\n",
        "    for s in range(num_block):\n",
        "      if s == 0:\n",
        "        if expansion*channel_out != self.in_channels: # test if the in channel and out channel are the same\n",
        "          layers.append(block(self.in_channels, channel_out, downsample = True))\n",
        "        else:\n",
        "          layers.append(block(channel_out, channel_out))\n",
        "        self.in_channels = channel_out\n",
        "      else:\n",
        "        layers.append(block(self.in_channels*expansion, channel_out, stride, downsample = True))\n",
        "    self.in_channels = channel_out * expansion\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x, return_embedding = False):\n",
        "\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool(x)\n",
        "\n",
        "    x = self.layer1(x)  \n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.layer4(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.avgpool(x)\n",
        "   \n",
        "    x = self.flatten(x)\n",
        "    \n",
        "    embedding_out = self.relu(self.linear(x))\n",
        "\n",
        "    output = self.linear_output(x)\n",
        "    if return_embedding:\n",
        "      return embedding_out, output\n",
        "    else:\n",
        "      return output\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ccs0vUSLqS1"
      },
      "source": [
        "\n",
        "def ResNet34(num_classes, feat_dim):\n",
        "  return ResNet(SimpleResidualBlock, [3,4,6,3],  num_classes, feat_dim)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slp5a0feMSn4"
      },
      "source": [
        "def training(epoch, network):\n",
        "  for epoch in range(numEpochs):\n",
        "      \n",
        "      network.train()\n",
        "      avg_loss = 0.0\n",
        "      num_correct = 0\n",
        "\n",
        "      for batch_num, (x, y) in enumerate(train_dataloader):\n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          x, y = x.to(device), y.to(device)\n",
        "\n",
        "          outputs = network(x)\n",
        "\n",
        "          loss = criterion(outputs, y.long())\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          \n",
        "          avg_loss += loss.item()\n",
        "          if batch_num % 99 == 1:\n",
        "              print('loss', avg_loss/100)\n",
        "              avg_loss = 0.0\n",
        "       \n",
        "          #print(torch.argmax(outputs, axis = 1)[0],y[0])\n",
        "          num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "      \n",
        "      \n",
        "      print('Epoch: {}, Training Accuracy: {:.2f}'.format(epoch, num_correct / len(train_dataset)))\n",
        "      print('train loss',avg_loss)\n",
        "      \n",
        "      # Validate\n",
        "      network.eval()\n",
        "      num_correct = 0\n",
        "      avg_loss = 0.0\n",
        "      with torch.no_grad():\n",
        "        for batch_num, (x, y) in enumerate(val_dataloader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            outputs = network(x)\n",
        "            loss = criterion(outputs, y.long())\n",
        "            avg_loss += loss.item()/len(val_dataloader)\n",
        "            \n",
        "            if batch_num % 99 == 1:\n",
        "              print('loss', avg_loss)\n",
        "              avg_loss = 0.0\n",
        "            \n",
        "            #print(torch.argmax(outputs, axis = 1)[0],y[0])\n",
        "            num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "        print('val loss', avg_loss)\n",
        "        print('Epoch: {}, Validation Accuracy: {:.2f}'.format(epoch, num_correct / len(val_dataset)))\n",
        "        acc = num_correct / len(val_dataset)\n",
        "        scheduler.step(acc)\n",
        "        if num_correct/len(val_dataset) >= 0.7:\n",
        "          torch.save(network,'resnet_0.7x2')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73mKjFEd_StJ"
      },
      "source": [
        "# tuning hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ePF_TixB0J0A",
        "outputId": "ecc9757c-803c-44c1-9787-469bb39b25b0"
      },
      "source": [
        "#different feature dimensions\n",
        "num_classes = len(train_dataset.classes)\n",
        "feat_dim_list = [1200]\n",
        "\n",
        "for feat_dim in feat_dim_list:\n",
        "  print(feat_dim)\n",
        "  \n",
        "  learningRate = 0.1\n",
        "  \n",
        "  numEpochs = 25\n",
        "\n",
        "  in_features = 3 # RGB channels\n",
        "\n",
        "  weightDecay = 5e-5 #5e-5\n",
        "\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  \n",
        "  network = ResNet34(num_classes, feat_dim)\n",
        "  network = network.to(device)\n",
        "  \n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(network.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)\n",
        "  \n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.2)\n",
        "  training(numEpochs, network)\n",
        "  \n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1200\n",
            "loss 0.1660374927520752\n",
            "loss 8.218052349090577\n",
            "loss 8.170220584869385\n",
            "loss 8.123944873809814\n",
            "loss 8.066201248168944\n",
            "loss 8.002848262786864\n",
            "loss 7.963303184509277\n",
            "loss 7.895539546012879\n",
            "loss 7.834396810531616\n",
            "loss 7.787184309959412\n",
            "loss 7.702411684989929\n",
            "loss 7.639333424568176\n",
            "loss 7.593445725440979\n",
            "loss 7.536490778923035\n",
            "loss 7.492283821105957\n",
            "loss 7.437731657028198\n",
            "loss 7.372276301383972\n",
            "loss 7.325784769058227\n",
            "loss 7.281984672546387\n",
            "loss 7.201012101173401\n",
            "loss 7.173785462379455\n",
            "loss 7.105573892593384\n",
            "loss 7.0541437196731565\n",
            "loss 7.024293284416199\n",
            "loss 6.926671710014343\n",
            "loss 6.881153025627136\n",
            "loss 6.810311598777771\n",
            "loss 6.756075692176819\n",
            "loss 6.690614399909973\n",
            "loss 6.635176043510437\n",
            "loss 6.5747545719146725\n",
            "Epoch: 0, Training Accuracy: 0.01\n",
            "train loss 13.199355602264404\n",
            "loss 0.2178756168910435\n",
            "val loss 6.673875452980164\n",
            "Epoch: 0, Validation Accuracy: 0.01\n",
            "loss 0.13135613918304442\n",
            "loss 6.463390817642212\n",
            "loss 6.4366775369644165\n",
            "loss 6.389204244613648\n",
            "loss 6.329658374786377\n",
            "loss 6.26291886806488\n",
            "loss 6.232704396247864\n",
            "loss 6.189112176895142\n",
            "loss 6.125583953857422\n",
            "loss 6.053979277610779\n",
            "loss 6.001370115280151\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-8398afee77e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;31m#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumEpochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-5a40f372c93b>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(epoch, model)\u001b[0m\n\u001b[1;32m     19\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m           \u001b[0mavg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mbatch_num\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m99\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZkEMV9T7kle",
        "outputId": "c84d3b23-46dd-4457-a595-a03120166e58"
      },
      "source": [
        "with torch.no_grad():\n",
        "        for batch_num, (x, y) in enumerate(val_dataloader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            outputs = network_val(x)\n",
        "            loss = criterion(outputs, y.long())\n",
        "            avg_loss += loss.item()/len(val_dataloader)\n",
        "            \n",
        "            if batch_num % 99 == 1:\n",
        "              print('loss', avg_loss)\n",
        "              avg_loss = 0.0\n",
        "            \n",
        "            #print(torch.argmax(outputs, axis = 1)[0],y[0])\n",
        "            num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "        print('val loss', avg_loss)\n",
        "        print('Validation Accuracy: {:.2f}'.format(num_correct / len(val_dataset)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 6.145896334496756\n",
            "val loss 1.0863212952538142\n",
            "Validation Accuracy: 0.80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_LuPRMyFelo"
      },
      "source": [
        "# testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbkvUZ1yFhPc"
      },
      "source": [
        "#read the test image\n",
        "\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from PIL import Image\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, file_list):\n",
        "        self.file_list = file_list\n",
        "        self.imgfiles = [f for f in listdir(self.file_list) if isfile(join(self.file_list, f))]\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgfiles)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        fname = self.file_list + '/'+ self.imgfiles[index]\n",
        "        \n",
        "        img = Image.open(fname)\n",
        "        img = torchvision.transforms.ToTensor()(img)\n",
        "\n",
        "        return img,self.imgfiles[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLJqtU36GeT_"
      },
      "source": [
        "testset = ImageDataset('test_data')\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=128, \n",
        "                                               shuffle=False, num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCeDP_5fJ3EC"
      },
      "source": [
        "network.eval()\n",
        "preds_total = []\n",
        "names = []\n",
        "with torch.no_grad():\n",
        "  for X_test, name in test_loader:\n",
        "   # print(name)\n",
        "    names.append(name)\n",
        "    X_test = X_test.to(device='cuda')\n",
        "    y_hat = network(X_test)\n",
        "    \n",
        "    y_hat = y_hat.cpu()\n",
        "    y_hat = y_hat.detach().numpy()\n",
        "    \n",
        "    preds = np.argmax(y_hat, axis = 1)\n",
        "   \n",
        "    preds = preds.reshape(-1,1)\n",
        "    preds_total.append(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laGs6UM1eJar",
        "outputId": "a4b1bec5-2ac3-422d-e224-1220a18864d3"
      },
      "source": [
        "dictclass = train_dataset.class_to_idx\n",
        "dictclass.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INF6OOpL6lJc",
        "outputId": "009fbab2-a36a-4f83-b27f-84a81ca7bd9f"
      },
      "source": [
        "dictclass.values()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgY2812K2q-H"
      },
      "source": [
        "finalnames = []\n",
        "for k in names:\n",
        "  #print(k)\n",
        "\n",
        "  for n in range(0,len(k)):\n",
        "    #print(k[n])\n",
        "\n",
        "    finalnames.append(k[n])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9UDamSWLYDX",
        "outputId": "a8d187bd-7df6-45ac-cdc5-2eb36aba7873"
      },
      "source": [
        "\n",
        "preds_final = []\n",
        "for i in preds_total:\n",
        "  for t in i:\n",
        "    \n",
        "    add = [key for key, val in dictclass.items() if val == t]\n",
        "    #print(add[0], t)\n",
        "    preds_final.append(int(add[0]))\n",
        "len(preds_final)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo6gOqp5LglC"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({'id':finalnames, \"label\":preds_final})\n",
        "df.to_csv(r\"submissionp1.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK86eHntME54",
        "outputId": "5928dbf4-6c70-486f-d534-f2af0bec45b4"
      },
      "source": [
        "!kaggle competitions submit -c 11785-spring2021-hw2p2s1-face-classification -f submissionp1.csv -m \"Message\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% 106k/106k [00:08<00:00, 12.5kB/s]\n",
            "Successfully submitted to 11785-Spring2021-HW2P2S1-Face-Classification"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DjfPyHKCWKV"
      },
      "source": [
        "del train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cN84_hGcbCE"
      },
      "source": [
        "# task2 pre-processing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld1FSsoDU0Xc"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0DGTrduWt0Z",
        "outputId": "7bb6aad2-5295-41c3-8904-9809629d88e0"
      },
      "source": [
        "%cd competitions/11785-spring2021-hw2p2s2-face-verification\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/competitions/11785-spring2021-hw2p2s2-face-verification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9cmvt0tXAUs",
        "outputId": "68a8ca3a-4f21-411e-cae6-49d36e2fdda3"
      },
      "source": [
        "!unzip 11785-spring2021-hw2p2s2-face-verification.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  inflating: verification_data/00064100.jpg  \n",
            "  inflating: verification_data/00064101.jpg  \n",
            "  inflating: verification_data/00064102.jpg  \n",
            "  inflating: verification_data/00069096.jpg  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23dvSVpcZPKH",
        "outputId": "813e1c61-8f8a-49cd-dc69-5ad32b1eca0b"
      },
      "source": [
        "%cd 11785-spring2021-hw2p2s2-face-verification"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/competitions/11785-spring2021-hw2p2s2-face-verification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkc-kdQgYnfw"
      },
      "source": [
        "from PIL import Image\n",
        "import PIL.ImageOps  \n",
        "import pandas as pd\n",
        "dir = 'verification_pairs_val.txt'\n",
        "compare = pd.read_csv(dir, header = None, delimiter = \"\\t\")\n",
        "text = []\n",
        "for c in compare[0]:\n",
        "  i1, i2, l = c.split(\" \")\n",
        "  text.append([i1,i2,l])\n",
        "\n",
        "  \n",
        "class TrainDataset(Dataset):\n",
        "    \n",
        "    def __init__(self,imageFolderDataset,text,transform=None,should_invert=False, test = False):\n",
        "        \n",
        "        \n",
        "        self.imageFolderDataset = imageFolderDataset    \n",
        "        self.transform = transform\n",
        "        self.should_invert = should_invert\n",
        "        self.image = [[t[0],t[1]] for t in text]\n",
        "        self.label = [t[2] for t in text]\n",
        "        self.test = test\n",
        "\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "\n",
        "        dirname0 =  text[index][0]\n",
        "        dirname1 =  text[index][1]\n",
        "        label = text[index][2]\n",
        "       \n",
        "        img0 = Image.open(dirname0)\n",
        "        img1 = Image.open(dirname1)\n",
        "        \n",
        "        \n",
        "        img0 = torchvision.transforms.ColorJitter(hue=.05, saturation=.05)(img0) \n",
        "        img0 = torchvision.transforms.RandomHorizontalFlip()(img0)\n",
        "        img0 = transforms.ToTensor()(img0)\n",
        "        img1 = transforms.ToTensor()(img1)\n",
        "        img1 = torchvision.transforms.ColorJitter(hue=.05, saturation=.05)(img1) \n",
        "        img1 = torchvision.transforms.RandomHorizontalFlip()(img1)\n",
        "        \n",
        "      \n",
        "        if self.should_invert:\n",
        "            img0 = PIL.ImageOps.invert(img0)\n",
        "            img1 = PIL.ImageOps.invert(img1)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img0 = self.transform(img0)\n",
        "            img1 = self.transform(img1)\n",
        "\n",
        "    \n",
        "\n",
        "        if self.test == False:\n",
        "          l = torch.from_numpy(np.array(label, dtype = np.float32))\n",
        "          return img0, img1, l\n",
        "        else:\n",
        "          return img0, img1 ,0\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4npTWy9YqsD"
      },
      "source": [
        "\n",
        "train_dataset = TrainDataset(dir, text)\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                        shuffle=True,\n",
        "                        num_workers=0,\n",
        "                        batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgmEMcewEX6n"
      },
      "source": [
        "# task2 Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGP4rBBC9dC9"
      },
      "source": [
        "compute_sim = nn.CosineSimilarity(dim=0)\n",
        "#print(\"CS b/n two images of class 0: {:.4f}\".format(compute_sim(feats_0, feats_1)))\n",
        "def testVerify(model, train_dataloader):\n",
        "    similarity = np.array([])\n",
        "    true = np.array([])\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (img0, img1, targets) in enumerate(train_dataloader):\n",
        "            img0, img1, targets = img0.cuda(), img1.cuda(), targets.cuda()\n",
        "          \n",
        "          \n",
        "            feats_0 = model(img0, return_embedding=True)[0].squeeze(0)\n",
        "            feats_1 = model(img1, return_embedding=True)[0].squeeze(0)\n",
        "      \n",
        "            sim = compute_sim(feats_0,feats_1)\n",
        "                     \n",
        "            similarity = np.concatenate((similarity, sim.cpu().numpy().reshape(-1)))\n",
        "\n",
        "            true = np.concatenate((true, targets.cpu().numpy().reshape(-1)))\n",
        "            \n",
        "            del img0\n",
        "            del img1\n",
        "            del targets\n",
        "    return similarity, true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CC2uy0-OZjoT",
        "outputId": "ec47d8c5-54e6-4201-e0b8-38e9d4b57269"
      },
      "source": [
        "%cd .. \n",
        "%cd 11785-spring2021-hw2p2s2-face-verification\n",
        "similarity, true = testVerify(network, train_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/competitions\n",
            "/content/gdrive/MyDrive/competitions/11785-spring2021-hw2p2s2-face-verification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qiz6wsC0cuNd",
        "outputId": "3fae7d97-aaa7-4463-9c0a-3f4c0237ef06"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "auc = roc_auc_score(true, similarity)\n",
        "print(auc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8960510933270018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYL8P2haGIIJ"
      },
      "source": [
        "# task2 test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQP0i74yXdpb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c9be7e5-d02f-47fa-c7a1-27302756cc10"
      },
      "source": [
        "%cd .. \n",
        "%cd 11785-spring2021-hw2p2s1-face-classification\n",
        "network = torch.load('resnet_0.7x2')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/competitions\n",
            "/content/gdrive/MyDrive/competitions/11785-spring2021-hw2p2s1-face-classification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMqeKlJMo1yA"
      },
      "source": [
        "dir = 'verification_pairs_test.txt'\n",
        "text = []\n",
        "compare = pd.read_csv(dir, header = None, delimiter = \"\\t\")\n",
        "for c in compare[0]:\n",
        "  i1, i2 = c.split(\" \")\n",
        "  text.append([i1,i2,0])\n",
        "\n",
        "test_dataset = TrainDataset(dir, text, test = True)\n",
        "test_dataloader = DataLoader(test_dataset,\n",
        "                        shuffle=False,\n",
        "                        num_workers=0,\n",
        "                        batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dndedocDs4lz"
      },
      "source": [
        "\n",
        "def testVerify_final(model, train_dataloader):\n",
        "    similarity = np.array([])\n",
        "    true = np.array([])\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (img0, img1, targets) in enumerate(train_dataloader):\n",
        "            img0, img1, targets = img0.cuda(), img1.cuda(), targets.cuda()\n",
        "            # find cos similarity between embeddings\n",
        "            #print(img0.shape)\n",
        "\n",
        "          \n",
        "            feats_0 = model(img0, return_embedding=True)[0].squeeze(0)\n",
        "            feats_1 = model(img1, return_embedding=True)[0].squeeze(0)\n",
        "      \n",
        "            sim = compute_sim(feats_0,feats_1)\n",
        "           # print('sim', sim)\n",
        "\n",
        "            similarity = np.concatenate((similarity, sim.cpu().numpy().reshape(-1)))\n",
        "    \n",
        "            \n",
        "            del img0\n",
        "            del img1\n",
        "            del targets\n",
        "    return similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4djGKJWtP5x"
      },
      "source": [
        "similarity = testVerify_final(network, test_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dcf6piQ4yidT"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({'Id':compare[0], \"Category\":similarity})\n",
        "df.to_csv(r\"submission4.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xUyiPrqy4Yh",
        "outputId": "d9394479-f2b1-4b95-bc4f-3700ceee1536"
      },
      "source": [
        "!kaggle competitions submit -c 11785-spring2021-hw2p2s2-face-verification -f submission4.csv -m \"Message\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "100% 4.02M/4.02M [00:03<00:00, 1.09MB/s]\n",
            "Successfully submitted to 11785-Spring2021-HW2P2S2-Face-Verification"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
